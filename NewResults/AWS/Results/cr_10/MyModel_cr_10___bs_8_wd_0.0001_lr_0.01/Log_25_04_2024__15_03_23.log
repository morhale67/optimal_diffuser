2024-04-25 15:03:23,786 This is a summery of the run:
2024-04-25 15:03:23,786 Batch size for this run: 8
2024-04-25 15:03:23,786 Size of original image: 32 X 32
2024-04-25 15:03:23,786 number of masks: 102
2024-04-25 15:03:23,786 Compression ratio: 10
2024-04-25 15:03:23,786 epochs : 40
2024-04-25 15:03:23,786 one learning rate: 0.01
2024-04-25 15:03:23,786 optimizer: adam
2024-04-25 15:03:23,786 weight_decay: 0.0001
2024-04-25 15:03:23,786 ***************************************************************************


2024-04-25 15:03:23,786 learning rate: 0.01
2024-04-25 15:03:26,920 Epoch number 0, batch number 0/10:       batch loss 0.08349569886922836
2024-04-25 15:03:27,580 Epoch number 0, batch number 1/10:       batch loss 0.45792123675346375
2024-04-25 15:03:28,069 Epoch number 0, batch number 2/10:       batch loss 0.7495669722557068
2024-04-25 15:03:28,567 Epoch number 0, batch number 3/10:       batch loss 0.7022117376327515
2024-04-25 15:03:29,045 Epoch number 0, batch number 4/10:       batch loss 0.7718048095703125
2024-04-25 15:03:29,528 Epoch number 0, batch number 5/10:       batch loss 0.7090556025505066
2024-04-25 15:03:30,078 Epoch number 0, batch number 6/10:       batch loss 0.7386912107467651
2024-04-25 15:03:30,556 Epoch number 0, batch number 7/10:       batch loss 0.8201159238815308
2024-04-25 15:03:31,045 Epoch number 0, batch number 8/10:       batch loss 0.7140867114067078
2024-04-25 15:03:31,554 Epoch number 0, batch number 9/10:       batch loss 0.6816269755363464
2024-04-25 15:03:32,767 Epoch number 0, batch number 0/2:       batch loss 0.5785634517669678
2024-04-25 15:03:33,470 Epoch number 0, batch number 1/2:       batch loss 0.6923232078552246
2024-04-25 15:03:33,549 Epoch: 1 	Training Loss: 0.080357
2024-04-25 15:03:33,550 Time for epoch 1 : 9 sec
2024-04-25 15:03:33,550 lr for epoch 1 is 0.01000
2024-04-25 15:03:35,011 Epoch number 1, batch number 0/10:       batch loss 0.7149534225463867
2024-04-25 15:03:35,730 Epoch number 1, batch number 1/10:       batch loss 0.6539188027381897
2024-04-25 15:03:36,225 Epoch number 1, batch number 2/10:       batch loss 0.6763699650764465
2024-04-25 15:03:36,735 Epoch number 1, batch number 3/10:       batch loss 0.6486228108406067
2024-04-25 15:03:37,222 Epoch number 1, batch number 4/10:       batch loss 0.6071755886077881
2024-04-25 15:03:37,716 Epoch number 1, batch number 5/10:       batch loss 0.7322908639907837
2024-04-25 15:03:38,207 Epoch number 1, batch number 6/10:       batch loss 0.6445476412773132
2024-04-25 15:03:38,694 Epoch number 1, batch number 7/10:       batch loss 0.6678910255432129
2024-04-25 15:03:39,173 Epoch number 1, batch number 8/10:       batch loss 0.6811524629592896
2024-04-25 15:03:39,683 Epoch number 1, batch number 9/10:       batch loss 0.5930226445198059
2024-04-25 15:03:40,868 Epoch number 1, batch number 0/2:       batch loss 0.6762096881866455
2024-04-25 15:03:41,546 Epoch number 1, batch number 1/2:       batch loss 0.6286511421203613
2024-04-25 15:03:41,663 Epoch: 2 	Training Loss: 0.082749
2024-04-25 15:03:41,663 Time for epoch 2 : 8 sec
2024-04-25 15:03:41,663 lr for epoch 2 is 0.01000
2024-04-25 15:03:43,110 Epoch number 2, batch number 0/10:       batch loss 0.6844652891159058
2024-04-25 15:03:43,747 Epoch number 2, batch number 1/10:       batch loss 0.6006922721862793
2024-04-25 15:03:44,238 Epoch number 2, batch number 2/10:       batch loss 0.6613655686378479
2024-04-25 15:03:44,718 Epoch number 2, batch number 3/10:       batch loss 0.7313492894172668
2024-04-25 15:03:45,201 Epoch number 2, batch number 4/10:       batch loss 0.6509191989898682
2024-04-25 15:03:45,676 Epoch number 2, batch number 5/10:       batch loss 0.6500752568244934
2024-04-25 15:03:46,171 Epoch number 2, batch number 6/10:       batch loss 0.6953589916229248
2024-04-25 15:03:46,714 Epoch number 2, batch number 7/10:       batch loss 0.6616175174713135
2024-04-25 15:03:47,204 Epoch number 2, batch number 8/10:       batch loss 0.6650841236114502
2024-04-25 15:03:47,692 Epoch number 2, batch number 9/10:       batch loss 0.7365002036094666
2024-04-25 15:03:48,856 Epoch number 2, batch number 0/2:       batch loss 0.5728529691696167
2024-04-25 15:03:49,530 Epoch number 2, batch number 1/2:       batch loss 0.7416249513626099
2024-04-25 15:03:49,619 Epoch: 3 	Training Loss: 0.084218
2024-04-25 15:03:49,619 Time for epoch 3 : 8 sec
2024-04-25 15:03:49,619 lr for epoch 3 is 0.01000
2024-04-25 15:03:51,155 Epoch number 3, batch number 0/10:       batch loss 0.6889532208442688
2024-04-25 15:03:51,838 Epoch number 3, batch number 1/10:       batch loss 0.7326131463050842
2024-04-25 15:03:52,357 Epoch number 3, batch number 2/10:       batch loss 0.6350694894790649
2024-04-25 15:03:52,853 Epoch number 3, batch number 3/10:       batch loss 0.6691897511482239
2024-04-25 15:03:53,333 Epoch number 3, batch number 4/10:       batch loss 0.6384595036506653
2024-04-25 15:03:53,807 Epoch number 3, batch number 5/10:       batch loss 0.6748278737068176
2024-04-25 15:03:54,278 Epoch number 3, batch number 6/10:       batch loss 0.7447455525398254
2024-04-25 15:03:54,767 Epoch number 3, batch number 7/10:       batch loss 0.6863129734992981
2024-04-25 15:03:55,249 Epoch number 3, batch number 8/10:       batch loss 0.8739275336265564
2024-04-25 15:03:55,756 Epoch number 3, batch number 9/10:       batch loss 0.8619702458381653
2024-04-25 15:03:57,032 Epoch number 3, batch number 0/2:       batch loss 0.735571026802063
2024-04-25 15:03:57,712 Epoch number 3, batch number 1/2:       batch loss 0.7113456726074219
2024-04-25 15:03:57,794 Epoch: 4 	Training Loss: 0.090076
2024-04-25 15:03:57,795 Time for epoch 4 : 8 sec
2024-04-25 15:03:57,795 lr for epoch 4 is 0.01000
2024-04-25 15:03:59,227 Epoch number 4, batch number 0/10:       batch loss 0.693345308303833
2024-04-25 15:03:59,913 Epoch number 4, batch number 1/10:       batch loss 0.6548017263412476
2024-04-25 15:04:00,417 Epoch number 4, batch number 2/10:       batch loss 0.7996220588684082
2024-04-25 15:04:00,899 Epoch number 4, batch number 3/10:       batch loss 0.7972941398620605
2024-04-25 15:04:01,381 Epoch number 4, batch number 4/10:       batch loss 0.6995410323143005
2024-04-25 15:04:01,876 Epoch number 4, batch number 5/10:       batch loss 0.7605028748512268
2024-04-25 15:04:02,361 Epoch number 4, batch number 6/10:       batch loss 0.822706937789917
2024-04-25 15:04:02,878 Epoch number 4, batch number 7/10:       batch loss 0.6730443239212036
2024-04-25 15:04:03,401 Epoch number 4, batch number 8/10:       batch loss 0.757623553276062
2024-04-25 15:04:03,881 Epoch number 4, batch number 9/10:       batch loss 0.6792724132537842
2024-04-25 15:04:04,977 Epoch number 4, batch number 0/2:       batch loss 0.6603627800941467
2024-04-25 15:04:05,664 Epoch number 4, batch number 1/2:       batch loss 0.6652051210403442
2024-04-25 15:04:05,788 Epoch: 5 	Training Loss: 0.091722
2024-04-25 15:04:05,789 Time for epoch 5 : 8 sec
2024-04-25 15:04:05,789 lr for epoch 5 is 0.01000
2024-04-25 15:04:07,204 Epoch number 5, batch number 0/10:       batch loss 0.7159770131111145
2024-04-25 15:04:07,838 Epoch number 5, batch number 1/10:       batch loss 0.6643205881118774
2024-04-25 15:04:08,354 Epoch number 5, batch number 2/10:       batch loss 0.7208397388458252
2024-04-25 15:04:08,838 Epoch number 5, batch number 3/10:       batch loss 0.776689887046814
2024-04-25 15:04:09,322 Epoch number 5, batch number 4/10:       batch loss 0.7115558385848999
2024-04-25 15:04:09,825 Epoch number 5, batch number 5/10:       batch loss 0.6227516531944275
2024-04-25 15:04:10,372 Epoch number 5, batch number 6/10:       batch loss 0.6358200311660767
2024-04-25 15:04:10,853 Epoch number 5, batch number 7/10:       batch loss 0.6650916337966919
2024-04-25 15:04:11,324 Epoch number 5, batch number 8/10:       batch loss 0.7534207701683044
2024-04-25 15:04:11,820 Epoch number 5, batch number 9/10:       batch loss 0.7429908514022827
2024-04-25 15:04:13,025 Epoch number 5, batch number 0/2:       batch loss 0.6616389751434326
2024-04-25 15:04:13,702 Epoch number 5, batch number 1/2:       batch loss 0.7016085982322693
2024-04-25 15:04:13,823 Epoch: 6 	Training Loss: 0.087618
2024-04-25 15:04:13,824 Time for epoch 6 : 8 sec
2024-04-25 15:04:13,824 lr for epoch 6 is 0.01000
2024-04-25 15:04:15,536 Epoch number 6, batch number 0/10:       batch loss 0.7165003418922424
2024-04-25 15:04:16,156 Epoch number 6, batch number 1/10:       batch loss 0.7484603524208069
2024-04-25 15:04:16,698 Epoch number 6, batch number 2/10:       batch loss 0.7220367193222046
2024-04-25 15:04:17,217 Epoch number 6, batch number 3/10:       batch loss 0.8121777772903442
2024-04-25 15:04:17,696 Epoch number 6, batch number 4/10:       batch loss 0.7137964963912964
2024-04-25 15:04:18,179 Epoch number 6, batch number 5/10:       batch loss 0.8147405385971069
2024-04-25 15:04:18,653 Epoch number 6, batch number 6/10:       batch loss 0.595174252986908
2024-04-25 15:04:19,140 Epoch number 6, batch number 7/10:       batch loss 0.6416199803352356
2024-04-25 15:04:19,614 Epoch number 6, batch number 8/10:       batch loss 0.8114023208618164
2024-04-25 15:04:20,090 Epoch number 6, batch number 9/10:       batch loss 0.7328383922576904
2024-04-25 15:04:21,341 Epoch number 6, batch number 0/2:       batch loss 0.6739864349365234
2024-04-25 15:04:22,016 Epoch number 6, batch number 1/2:       batch loss 0.7990443110466003
2024-04-25 15:04:22,134 Epoch: 7 	Training Loss: 0.091359
2024-04-25 15:04:22,134 Time for epoch 7 : 8 sec
2024-04-25 15:04:22,134 lr for epoch 7 is 0.01000
2024-04-25 15:04:23,687 Epoch number 7, batch number 0/10:       batch loss 0.8363029956817627
2024-04-25 15:04:24,349 Epoch number 7, batch number 1/10:       batch loss 0.8164753913879395
2024-04-25 15:04:24,882 Epoch number 7, batch number 2/10:       batch loss 0.8274092078208923
2024-04-25 15:04:25,368 Epoch number 7, batch number 3/10:       batch loss 0.7656230330467224
2024-04-25 15:04:25,848 Epoch number 7, batch number 4/10:       batch loss 0.7368215918540955
2024-04-25 15:04:26,323 Epoch number 7, batch number 5/10:       batch loss 0.6748147010803223
2024-04-25 15:04:26,805 Epoch number 7, batch number 6/10:       batch loss 0.7739136219024658
2024-04-25 15:04:27,287 Epoch number 7, batch number 7/10:       batch loss 0.7547211050987244
2024-04-25 15:04:27,768 Epoch number 7, batch number 8/10:       batch loss 0.756316602230072
2024-04-25 15:04:28,239 Epoch number 7, batch number 9/10:       batch loss 0.7392659783363342
2024-04-25 15:04:29,442 Epoch number 7, batch number 0/2:       batch loss 0.7124102115631104
2024-04-25 15:04:30,130 Epoch number 7, batch number 1/2:       batch loss 0.7712354063987732
2024-04-25 15:04:30,236 Epoch: 8 	Training Loss: 0.096021
2024-04-25 15:04:30,236 Time for epoch 8 : 8 sec
2024-04-25 15:04:30,236 lr for epoch 8 is 0.01000
2024-04-25 15:04:31,642 Epoch number 8, batch number 0/10:       batch loss 0.795905590057373
2024-04-25 15:04:32,300 Epoch number 8, batch number 1/10:       batch loss 0.7083983421325684
2024-04-25 15:04:32,811 Epoch number 8, batch number 2/10:       batch loss 0.7905965447425842
2024-04-25 15:04:33,298 Epoch number 8, batch number 3/10:       batch loss 0.7152541875839233
2024-04-25 15:04:33,780 Epoch number 8, batch number 4/10:       batch loss 0.7834182977676392
2024-04-25 15:04:34,255 Epoch number 8, batch number 5/10:       batch loss 0.9031404256820679
2024-04-25 15:04:34,727 Epoch number 8, batch number 6/10:       batch loss 0.8186416625976562
2024-04-25 15:04:35,235 Epoch number 8, batch number 7/10:       batch loss 0.7994837164878845
2024-04-25 15:04:35,718 Epoch number 8, batch number 8/10:       batch loss 0.8000151515007019
2024-04-25 15:04:36,201 Epoch number 8, batch number 9/10:       batch loss 0.9041145443916321
2024-04-25 15:04:37,408 Epoch number 8, batch number 0/2:       batch loss 0.762316882610321
2024-04-25 15:04:38,083 Epoch number 8, batch number 1/2:       batch loss 0.7670654654502869
2024-04-25 15:04:38,190 Epoch: 9 	Training Loss: 0.100237
2024-04-25 15:04:38,190 Time for epoch 9 : 8 sec
2024-04-25 15:04:38,190 lr for epoch 9 is 0.01000
2024-04-25 15:04:39,693 Epoch number 9, batch number 0/10:       batch loss 0.8152559995651245
2024-04-25 15:04:40,307 Epoch number 9, batch number 1/10:       batch loss 0.7604701519012451
2024-04-25 15:04:40,826 Epoch number 9, batch number 2/10:       batch loss 0.6837213635444641
2024-04-25 15:04:41,344 Epoch number 9, batch number 3/10:       batch loss 0.794934868812561
2024-04-25 15:04:41,831 Epoch number 9, batch number 4/10:       batch loss 0.8116191625595093
2024-04-25 15:04:42,344 Epoch number 9, batch number 5/10:       batch loss 0.7499155402183533
2024-04-25 15:04:42,823 Epoch number 9, batch number 6/10:       batch loss 0.8312923312187195
2024-04-25 15:04:43,313 Epoch number 9, batch number 7/10:       batch loss 0.9384267330169678
2024-04-25 15:04:43,796 Epoch number 9, batch number 8/10:       batch loss 0.7863864898681641
2024-04-25 15:04:44,272 Epoch number 9, batch number 9/10:       batch loss 0.8297141790390015
2024-04-25 15:04:45,478 Epoch number 9, batch number 0/2:       batch loss 0.763664722442627
2024-04-25 15:04:46,160 Epoch number 9, batch number 1/2:       batch loss 0.8084945678710938
2024-04-25 15:04:46,279 Epoch: 10 	Training Loss: 0.100022
2024-04-25 15:04:46,279 Time for epoch 10 : 8 sec
2024-04-25 15:04:46,279 lr for epoch 10 is 0.01000
2024-04-25 15:04:47,661 Epoch number 10, batch number 0/10:       batch loss 0.8235243558883667
2024-04-25 15:04:48,384 Epoch number 10, batch number 1/10:       batch loss 0.7632886171340942
2024-04-25 15:04:48,909 Epoch number 10, batch number 2/10:       batch loss 0.8818399310112
2024-04-25 15:04:49,431 Epoch number 10, batch number 3/10:       batch loss 0.7040484547615051
2024-04-25 15:04:49,959 Epoch number 10, batch number 4/10:       batch loss 0.7673675417900085
2024-04-25 15:04:50,439 Epoch number 10, batch number 5/10:       batch loss 0.7436079382896423
2024-04-25 15:04:50,924 Epoch number 10, batch number 6/10:       batch loss 0.8591096997261047
2024-04-25 15:04:51,404 Epoch number 10, batch number 7/10:       batch loss 0.7827476263046265
2024-04-25 15:04:51,884 Epoch number 10, batch number 8/10:       batch loss 0.8674271106719971
2024-04-25 15:04:52,365 Epoch number 10, batch number 9/10:       batch loss 0.7562295198440552
2024-04-25 15:04:53,582 Epoch number 10, batch number 0/2:       batch loss 0.7507596015930176
2024-04-25 15:04:54,242 Epoch number 10, batch number 1/2:       batch loss 0.7489097714424133
2024-04-25 15:04:54,349 Epoch: 11 	Training Loss: 0.099365
2024-04-25 15:04:54,349 Time for epoch 11 : 8 sec
2024-04-25 15:04:54,349 lr for epoch 11 is 0.01000
2024-04-25 15:04:55,868 Epoch number 11, batch number 0/10:       batch loss 0.8030512928962708
2024-04-25 15:04:56,460 Epoch number 11, batch number 1/10:       batch loss 0.877718448638916
2024-04-25 15:04:56,983 Epoch number 11, batch number 2/10:       batch loss 0.6196345090866089
2024-04-25 15:04:57,518 Epoch number 11, batch number 3/10:       batch loss 0.8831546902656555
2024-04-25 15:04:57,995 Epoch number 11, batch number 4/10:       batch loss 0.8314521312713623
2024-04-25 15:04:58,492 Epoch number 11, batch number 5/10:       batch loss 0.7117081880569458
2024-04-25 15:04:58,966 Epoch number 11, batch number 6/10:       batch loss 0.826888918876648
2024-04-25 15:04:59,438 Epoch number 11, batch number 7/10:       batch loss 0.8563091158866882
2024-04-25 15:04:59,914 Epoch number 11, batch number 8/10:       batch loss 0.8138933181762695
2024-04-25 15:05:00,397 Epoch number 11, batch number 9/10:       batch loss 0.8386402130126953
2024-04-25 15:05:01,745 Epoch number 11, batch number 0/2:       batch loss 0.8206085562705994
2024-04-25 15:05:02,410 Epoch number 11, batch number 1/2:       batch loss 0.8026149272918701
2024-04-25 15:05:02,504 Epoch: 12 	Training Loss: 0.100781
2024-04-25 15:05:02,504 Time for epoch 12 : 8 sec
2024-04-25 15:05:02,504 lr for epoch 12 is 0.01000
2024-04-25 15:05:03,940 Epoch number 12, batch number 0/10:       batch loss 0.8760613799095154
2024-04-25 15:05:04,555 Epoch number 12, batch number 1/10:       batch loss 0.808739423751831
2024-04-25 15:05:05,123 Epoch number 12, batch number 2/10:       batch loss 0.8826203346252441
2024-04-25 15:05:05,731 Epoch number 12, batch number 3/10:       batch loss 0.8674314022064209
2024-04-25 15:05:06,253 Epoch number 12, batch number 4/10:       batch loss 0.7682353258132935
2024-04-25 15:05:06,796 Epoch number 12, batch number 5/10:       batch loss 0.6910437345504761
2024-04-25 15:05:07,310 Epoch number 12, batch number 6/10:       batch loss 0.7733011245727539
2024-04-25 15:05:07,827 Epoch number 12, batch number 7/10:       batch loss 0.745273232460022
2024-04-25 15:05:08,342 Epoch number 12, batch number 8/10:       batch loss 0.7599797248840332
2024-04-25 15:05:08,852 Epoch number 12, batch number 9/10:       batch loss 0.9128074645996094
2024-04-25 15:05:10,203 Epoch number 12, batch number 0/2:       batch loss 0.6800208687782288
2024-04-25 15:05:10,877 Epoch number 12, batch number 1/2:       batch loss 0.6843454837799072
2024-04-25 15:05:10,974 Epoch: 13 	Training Loss: 0.101069
2024-04-25 15:05:10,974 Time for epoch 13 : 8 sec
2024-04-25 15:05:10,975 lr for epoch 13 is 0.01000
2024-04-25 15:05:12,736 Epoch number 13, batch number 0/10:       batch loss 0.6727768182754517
2024-04-25 15:05:13,386 Epoch number 13, batch number 1/10:       batch loss 0.7979945540428162
2024-04-25 15:05:13,985 Epoch number 13, batch number 2/10:       batch loss 0.7414351105690002
2024-04-25 15:05:14,614 Epoch number 13, batch number 3/10:       batch loss 0.6807976961135864
2024-04-25 15:05:15,135 Epoch number 13, batch number 4/10:       batch loss 0.756528913974762
2024-04-25 15:05:15,663 Epoch number 13, batch number 5/10:       batch loss 0.6960925459861755
2024-04-25 15:05:16,189 Epoch number 13, batch number 6/10:       batch loss 0.7649901509284973
2024-04-25 15:05:16,720 Epoch number 13, batch number 7/10:       batch loss 0.6811241507530212
2024-04-25 15:05:17,315 Epoch number 13, batch number 8/10:       batch loss 0.6744728088378906
2024-04-25 15:05:17,847 Epoch number 13, batch number 9/10:       batch loss 0.6276811957359314
2024-04-25 15:05:19,182 Epoch number 13, batch number 0/2:       batch loss 0.6690255999565125
2024-04-25 15:05:19,854 Epoch number 13, batch number 1/2:       batch loss 0.8176021575927734
2024-04-25 15:05:19,971 Epoch: 14 	Training Loss: 0.088674
2024-04-25 15:05:19,971 Time for epoch 14 : 9 sec
2024-04-25 15:05:19,971 lr for epoch 14 is 0.01000
2024-04-25 15:05:21,427 Epoch number 14, batch number 0/10:       batch loss 0.6578109860420227
2024-04-25 15:05:22,130 Epoch number 14, batch number 1/10:       batch loss 0.6831814050674438
2024-04-25 15:05:22,699 Epoch number 14, batch number 2/10:       batch loss 0.7041757106781006
2024-04-25 15:05:23,995 Epoch number 14, batch number 3/10:       batch loss 1.7484252452850342
2024-04-25 15:05:25,725 Epoch number 14, batch number 4/10:       batch loss 2.1172995567321777
2024-04-25 15:05:26,375 Epoch number 14, batch number 5/10:       batch loss 0.9200077056884766
2024-04-25 15:05:27,001 Epoch number 14, batch number 6/10:       batch loss 1.1322822570800781
2024-04-25 15:05:27,625 Epoch number 14, batch number 7/10:       batch loss 1.07120680809021
2024-04-25 15:05:28,246 Epoch number 14, batch number 8/10:       batch loss 0.9760220646858215
2024-04-25 15:05:28,873 Epoch number 14, batch number 9/10:       batch loss 1.1013697385787964
2024-04-25 15:05:30,126 Epoch number 14, batch number 0/2:       batch loss 1.1302096843719482
2024-04-25 15:05:30,846 Epoch number 14, batch number 1/2:       batch loss 0.9383475184440613
2024-04-25 15:05:30,962 Epoch: 15 	Training Loss: 0.138897
2024-04-25 15:05:30,962 Time for epoch 15 : 11 sec
2024-04-25 15:05:30,962 lr for epoch 15 is 0.01000
2024-04-25 15:05:32,589 Epoch number 15, batch number 0/10:       batch loss 1.042543649673462
2024-04-25 15:05:33,384 Epoch number 15, batch number 1/10:       batch loss 0.9766374230384827
2024-04-25 15:05:34,030 Epoch number 15, batch number 2/10:       batch loss 0.9840828776359558
2024-04-25 15:05:34,686 Epoch number 15, batch number 3/10:       batch loss 1.0244907140731812
2024-04-25 15:05:35,352 Epoch number 15, batch number 4/10:       batch loss 0.9736216068267822
2024-04-25 15:05:35,983 Epoch number 15, batch number 5/10:       batch loss 0.9040123224258423
2024-04-25 15:05:36,610 Epoch number 15, batch number 6/10:       batch loss 1.0536681413650513
2024-04-25 15:05:37,238 Epoch number 15, batch number 7/10:       batch loss 1.008713722229004
2024-04-25 15:05:37,869 Epoch number 15, batch number 8/10:       batch loss 1.0047234296798706
2024-04-25 15:05:38,493 Epoch number 15, batch number 9/10:       batch loss 1.112162709236145
2024-04-25 15:05:39,905 Epoch number 15, batch number 0/2:       batch loss 1.0069266557693481
2024-04-25 15:05:40,636 Epoch number 15, batch number 1/2:       batch loss 0.9849987030029297
2024-04-25 15:05:40,743 Epoch: 16 	Training Loss: 0.126058
2024-04-25 15:05:40,744 Time for epoch 16 : 10 sec
2024-04-25 15:05:40,744 lr for epoch 16 is 0.01000
2024-04-25 15:05:42,291 Epoch number 16, batch number 0/10:       batch loss 1.0060373544692993
2024-04-25 15:05:43,138 Epoch number 16, batch number 1/10:       batch loss 1.0992884635925293
2024-04-25 15:05:43,801 Epoch number 16, batch number 2/10:       batch loss 1.0198112726211548
2024-04-25 15:05:44,442 Epoch number 16, batch number 3/10:       batch loss 1.2252649068832397
2024-04-25 15:05:45,072 Epoch number 16, batch number 4/10:       batch loss 0.91092848777771
2024-04-25 15:05:45,704 Epoch number 16, batch number 5/10:       batch loss 1.2470383644104004
2024-04-25 15:05:46,336 Epoch number 16, batch number 6/10:       batch loss 1.1072062253952026
2024-04-25 15:05:46,970 Epoch number 16, batch number 7/10:       batch loss 1.2245773077011108
2024-04-25 15:05:47,599 Epoch number 16, batch number 8/10:       batch loss 1.1095247268676758
2024-04-25 15:05:48,230 Epoch number 16, batch number 9/10:       batch loss 0.7954883575439453
2024-04-25 15:05:49,476 Epoch number 16, batch number 0/2:       batch loss 1.0061078071594238
2024-04-25 15:05:50,202 Epoch number 16, batch number 1/2:       batch loss 0.9882580041885376
2024-04-25 15:05:50,309 Epoch: 17 	Training Loss: 0.134315
2024-04-25 15:05:50,310 Time for epoch 17 : 10 sec
2024-04-25 15:05:50,310 lr for epoch 17 is 0.01000
2024-04-25 15:05:51,911 Epoch number 17, batch number 0/10:       batch loss 0.9782149195671082
2024-04-25 15:05:52,771 Epoch number 17, batch number 1/10:       batch loss 1.0252901315689087
2024-04-25 15:05:53,432 Epoch number 17, batch number 2/10:       batch loss 1.1610652208328247
2024-04-25 15:05:54,129 Epoch number 17, batch number 3/10:       batch loss 1.164749264717102
2024-04-25 15:05:54,803 Epoch number 17, batch number 4/10:       batch loss 1.0765759944915771
2024-04-25 15:05:55,443 Epoch number 17, batch number 5/10:       batch loss 1.1559135913848877
2024-04-25 15:05:56,074 Epoch number 17, batch number 6/10:       batch loss 1.073821783065796
2024-04-25 15:05:56,724 Epoch number 17, batch number 7/10:       batch loss 1.0401421785354614
2024-04-25 15:05:57,351 Epoch number 17, batch number 8/10:       batch loss 1.1413657665252686
2024-04-25 15:05:57,987 Epoch number 17, batch number 9/10:       batch loss 0.8196791410446167
2024-04-25 15:05:59,271 Epoch number 17, batch number 0/2:       batch loss 0.999415397644043
2024-04-25 15:05:59,975 Epoch number 17, batch number 1/2:       batch loss 0.954707145690918
2024-04-25 15:06:00,081 Epoch: 18 	Training Loss: 0.132960
2024-04-25 15:06:00,081 Time for epoch 18 : 10 sec
2024-04-25 15:06:00,081 lr for epoch 18 is 0.01000
2024-04-25 15:06:01,710 Epoch number 18, batch number 0/10:       batch loss 0.8810756206512451
2024-04-25 15:06:02,262 Epoch number 18, batch number 1/10:       batch loss 0.6140366792678833
2024-04-25 15:06:02,732 Epoch number 18, batch number 2/10:       batch loss 0.6180269718170166
2024-04-25 15:06:03,221 Epoch number 18, batch number 3/10:       batch loss 0.5946338772773743
2024-04-25 15:06:03,734 Epoch number 18, batch number 4/10:       batch loss 0.6080983877182007
2024-04-25 15:06:04,258 Epoch number 18, batch number 5/10:       batch loss 1.2186994552612305
2024-04-25 15:06:04,712 Epoch number 18, batch number 6/10:       batch loss 0.6237927675247192
2024-04-25 15:06:05,200 Epoch number 18, batch number 7/10:       batch loss 0.5106260180473328
2024-04-25 15:06:05,686 Epoch number 18, batch number 8/10:       batch loss 0.6356692314147949
2024-04-25 15:06:06,167 Epoch number 18, batch number 9/10:       batch loss 0.6044490933418274
2024-04-25 15:06:07,530 Epoch number 18, batch number 0/2:       batch loss 0.5796079039573669
2024-04-25 15:06:08,205 Epoch number 18, batch number 1/2:       batch loss 0.5884805917739868
2024-04-25 15:06:08,326 Epoch: 19 	Training Loss: 0.086364
2024-04-25 15:06:08,326 Time for epoch 19 : 8 sec
2024-04-25 15:06:08,326 lr for epoch 19 is 0.01000
2024-04-25 15:06:09,818 Epoch number 19, batch number 0/10:       batch loss 0.6725732088088989
2024-04-25 15:06:10,472 Epoch number 19, batch number 1/10:       batch loss 0.6156973242759705
2024-04-25 15:06:10,970 Epoch number 19, batch number 2/10:       batch loss 0.6500592231750488
2024-04-25 15:06:11,530 Epoch number 19, batch number 3/10:       batch loss 0.6342867016792297
2024-04-25 15:06:12,066 Epoch number 19, batch number 4/10:       batch loss 0.6144295930862427
2024-04-25 15:06:12,540 Epoch number 19, batch number 5/10:       batch loss 0.5880898237228394
2024-04-25 15:06:13,017 Epoch number 19, batch number 6/10:       batch loss 0.5036010146141052
2024-04-25 15:06:13,503 Epoch number 19, batch number 7/10:       batch loss 0.5776339769363403
2024-04-25 15:06:13,993 Epoch number 19, batch number 8/10:       batch loss 0.5690044164657593
2024-04-25 15:06:14,473 Epoch number 19, batch number 9/10:       batch loss 0.48971953988075256
2024-04-25 15:06:15,728 Epoch number 19, batch number 0/2:       batch loss 0.5255489349365234
2024-04-25 15:06:16,390 Epoch number 19, batch number 1/2:       batch loss 0.5812947750091553
2024-04-25 15:06:16,507 Epoch: 20 	Training Loss: 0.073939
2024-04-25 15:06:16,507 Time for epoch 20 : 8 sec
2024-04-25 15:06:16,507 lr for epoch 20 is 0.01000
2024-04-25 15:06:18,008 Epoch number 20, batch number 0/10:       batch loss 0.5857334733009338
2024-04-25 15:06:18,607 Epoch number 20, batch number 1/10:       batch loss 0.5945425629615784
2024-04-25 15:06:19,111 Epoch number 20, batch number 2/10:       batch loss 0.6188483834266663
2024-04-25 15:06:19,659 Epoch number 20, batch number 3/10:       batch loss 0.5916547179222107
2024-04-25 15:06:20,147 Epoch number 20, batch number 4/10:       batch loss 0.49326151609420776
2024-04-25 15:06:20,640 Epoch number 20, batch number 5/10:       batch loss 0.4605349898338318
2024-04-25 15:06:21,132 Epoch number 20, batch number 6/10:       batch loss 0.5792410373687744
2024-04-25 15:06:21,612 Epoch number 20, batch number 7/10:       batch loss 0.47941455245018005
2024-04-25 15:06:22,099 Epoch number 20, batch number 8/10:       batch loss 0.5261270403862
2024-04-25 15:06:22,579 Epoch number 20, batch number 9/10:       batch loss 0.5507749319076538
2024-04-25 15:06:23,783 Epoch number 20, batch number 0/2:       batch loss 0.5259455442428589
2024-04-25 15:06:24,461 Epoch number 20, batch number 1/2:       batch loss 0.4943171441555023
2024-04-25 15:06:24,578 Epoch: 21 	Training Loss: 0.068502
2024-04-25 15:06:24,578 Time for epoch 21 : 8 sec
2024-04-25 15:06:24,578 lr for epoch 21 is 0.01000
2024-04-25 15:06:26,041 Epoch number 21, batch number 0/10:       batch loss 0.5219900608062744
2024-04-25 15:06:26,716 Epoch number 21, batch number 1/10:       batch loss 0.5274261236190796
2024-04-25 15:06:27,209 Epoch number 21, batch number 2/10:       batch loss 0.5224118828773499
2024-04-25 15:06:27,681 Epoch number 21, batch number 3/10:       batch loss 0.4700756371021271
2024-04-25 15:06:28,163 Epoch number 21, batch number 4/10:       batch loss 0.5093462467193604
2024-04-25 15:06:28,728 Epoch number 21, batch number 5/10:       batch loss 0.5449556112289429
2024-04-25 15:06:29,207 Epoch number 21, batch number 6/10:       batch loss 0.4816058874130249
2024-04-25 15:06:29,633 Epoch number 21, batch number 7/10:       batch loss 0.32716745138168335
2024-04-25 15:06:30,040 Epoch number 21, batch number 8/10:       batch loss 0.30596816539764404
2024-04-25 15:06:30,443 Epoch number 21, batch number 9/10:       batch loss 0.3420961797237396
2024-04-25 15:06:31,628 Epoch number 21, batch number 0/2:       batch loss 0.3137950003147125
2024-04-25 15:06:32,287 Epoch number 21, batch number 1/2:       batch loss 0.2437184453010559
2024-04-25 15:06:32,407 Epoch: 22 	Training Loss: 0.056913
2024-04-25 15:06:32,407 Time for epoch 22 : 8 sec
2024-04-25 15:06:32,407 lr for epoch 22 is 0.01000
2024-04-25 15:06:33,820 Epoch number 22, batch number 0/10:       batch loss 0.2523115277290344
2024-04-25 15:06:34,400 Epoch number 22, batch number 1/10:       batch loss 0.30729711055755615
2024-04-25 15:06:34,817 Epoch number 22, batch number 2/10:       batch loss 0.3239747881889343
2024-04-25 15:06:35,228 Epoch number 22, batch number 3/10:       batch loss 0.28708353638648987
2024-04-25 15:06:35,641 Epoch number 22, batch number 4/10:       batch loss 0.34626272320747375
2024-04-25 15:06:36,042 Epoch number 22, batch number 5/10:       batch loss 0.33911824226379395
2024-04-25 15:06:36,446 Epoch number 22, batch number 6/10:       batch loss 0.2780553996562958
2024-04-25 15:06:36,856 Epoch number 22, batch number 7/10:       batch loss 0.27132752537727356
2024-04-25 15:06:37,352 Epoch number 22, batch number 8/10:       batch loss 0.6060869097709656
2024-04-25 15:06:37,842 Epoch number 22, batch number 9/10:       batch loss 0.5380188226699829
2024-04-25 15:06:39,028 Epoch number 22, batch number 0/2:       batch loss 0.5541307926177979
2024-04-25 15:06:39,710 Epoch number 22, batch number 1/2:       batch loss 0.5387875437736511
2024-04-25 15:06:39,805 Epoch: 23 	Training Loss: 0.044369
2024-04-25 15:06:39,805 Time for epoch 23 : 7 sec
2024-04-25 15:06:39,805 lr for epoch 23 is 0.01000
2024-04-25 15:06:41,203 Epoch number 23, batch number 0/10:       batch loss 0.49951884150505066
2024-04-25 15:06:41,862 Epoch number 23, batch number 1/10:       batch loss 0.5631200075149536
2024-04-25 15:06:42,357 Epoch number 23, batch number 2/10:       batch loss 0.5742496252059937
2024-04-25 15:06:42,845 Epoch number 23, batch number 3/10:       batch loss 0.6246932148933411
2024-04-25 15:06:43,316 Epoch number 23, batch number 4/10:       batch loss 0.5124384164810181
2024-04-25 15:06:43,807 Epoch number 23, batch number 5/10:       batch loss 0.36758723855018616
2024-04-25 15:06:44,222 Epoch number 23, batch number 6/10:       batch loss 0.29981446266174316
2024-04-25 15:06:44,760 Epoch number 23, batch number 7/10:       batch loss 0.49445512890815735
2024-04-25 15:06:45,244 Epoch number 23, batch number 8/10:       batch loss 0.5269420146942139
2024-04-25 15:06:45,655 Epoch number 23, batch number 9/10:       batch loss 0.31180450320243835
2024-04-25 15:06:46,870 Epoch number 23, batch number 0/2:       batch loss 0.2212086319923401
2024-04-25 15:06:47,521 Epoch number 23, batch number 1/2:       batch loss 0.29944390058517456
2024-04-25 15:06:47,622 Epoch: 24 	Training Loss: 0.059683
2024-04-25 15:06:47,622 Time for epoch 24 : 8 sec
2024-04-25 15:06:47,622 lr for epoch 24 is 0.01000
2024-04-25 15:06:49,002 Epoch number 24, batch number 0/10:       batch loss 0.31428295373916626
2024-04-25 15:06:49,648 Epoch number 24, batch number 1/10:       batch loss 0.3499135971069336
2024-04-25 15:06:50,070 Epoch number 24, batch number 2/10:       batch loss 0.2891468405723572
2024-04-25 15:06:50,564 Epoch number 24, batch number 3/10:       batch loss 0.4295014441013336
2024-04-25 15:06:51,073 Epoch number 24, batch number 4/10:       batch loss 0.5523499846458435
2024-04-25 15:06:51,562 Epoch number 24, batch number 5/10:       batch loss 0.5553092360496521
2024-04-25 15:06:51,983 Epoch number 24, batch number 6/10:       batch loss 0.28481587767601013
2024-04-25 15:06:52,384 Epoch number 24, batch number 7/10:       batch loss 0.2710990011692047
2024-04-25 15:06:52,783 Epoch number 24, batch number 8/10:       batch loss 0.24251917004585266
2024-04-25 15:06:53,197 Epoch number 24, batch number 9/10:       batch loss 0.2461128979921341
2024-04-25 15:06:54,377 Epoch number 24, batch number 0/2:       batch loss 0.25229498744010925
2024-04-25 15:06:55,049 Epoch number 24, batch number 1/2:       batch loss 0.23323890566825867
2024-04-25 15:06:55,122 Epoch: 25 	Training Loss: 0.044188
2024-04-25 15:06:55,122 Time for epoch 25 : 8 sec
2024-04-25 15:06:55,122 lr for epoch 25 is 0.01000
2024-04-25 15:06:56,667 Epoch number 25, batch number 0/10:       batch loss 0.26209548115730286
2024-04-25 15:06:57,241 Epoch number 25, batch number 1/10:       batch loss 0.3083188235759735
2024-04-25 15:06:57,678 Epoch number 25, batch number 2/10:       batch loss 0.21414272487163544
2024-04-25 15:06:58,093 Epoch number 25, batch number 3/10:       batch loss 0.2694903314113617
2024-04-25 15:06:58,498 Epoch number 25, batch number 4/10:       batch loss 0.23776713013648987
2024-04-25 15:06:58,939 Epoch number 25, batch number 5/10:       batch loss 0.24845267832279205
2024-04-25 15:06:59,349 Epoch number 25, batch number 6/10:       batch loss 0.27535921335220337
2024-04-25 15:06:59,753 Epoch number 25, batch number 7/10:       batch loss 0.2207869589328766
2024-04-25 15:07:00,156 Epoch number 25, batch number 8/10:       batch loss 0.23015901446342468
2024-04-25 15:07:00,567 Epoch number 25, batch number 9/10:       batch loss 0.2575555145740509
2024-04-25 15:07:01,703 Epoch number 25, batch number 0/2:       batch loss 0.2089790254831314
2024-04-25 15:07:02,360 Epoch number 25, batch number 1/2:       batch loss 0.22158151865005493
2024-04-25 15:07:02,488 Epoch: 26 	Training Loss: 0.031552
2024-04-25 15:07:02,488 Time for epoch 26 : 7 sec
2024-04-25 15:07:02,488 lr for epoch 26 is 0.01000
2024-04-25 15:07:03,835 Epoch number 26, batch number 0/10:       batch loss 0.17242777347564697
2024-04-25 15:07:04,453 Epoch number 26, batch number 1/10:       batch loss 0.248764306306839
2024-04-25 15:07:04,883 Epoch number 26, batch number 2/10:       batch loss 0.2187114953994751
2024-04-25 15:07:05,361 Epoch number 26, batch number 3/10:       batch loss 0.19855308532714844
2024-04-25 15:07:05,771 Epoch number 26, batch number 4/10:       batch loss 0.2761756181716919
2024-04-25 15:07:06,178 Epoch number 26, batch number 5/10:       batch loss 0.22684608399868011
2024-04-25 15:07:06,584 Epoch number 26, batch number 6/10:       batch loss 0.21549588441848755
2024-04-25 15:07:06,997 Epoch number 26, batch number 7/10:       batch loss 0.18358759582042694
2024-04-25 15:07:07,411 Epoch number 26, batch number 8/10:       batch loss 0.24366995692253113
2024-04-25 15:07:07,825 Epoch number 26, batch number 9/10:       batch loss 0.19037267565727234
2024-04-25 15:07:09,121 Epoch number 26, batch number 0/2:       batch loss 0.14526154100894928
2024-04-25 15:07:09,778 Epoch number 26, batch number 1/2:       batch loss 0.21888212859630585
2024-04-25 15:07:09,894 Epoch: 27 	Training Loss: 0.027183
2024-04-25 15:07:09,894 Time for epoch 27 : 7 sec
2024-04-25 15:07:09,894 lr for epoch 27 is 0.01000
2024-04-25 15:07:11,228 Epoch number 27, batch number 0/10:       batch loss 0.17587590217590332
2024-04-25 15:07:11,809 Epoch number 27, batch number 1/10:       batch loss 0.18200305104255676
2024-04-25 15:07:12,240 Epoch number 27, batch number 2/10:       batch loss 0.21235932409763336
2024-04-25 15:07:12,767 Epoch number 27, batch number 3/10:       batch loss 0.4466842710971832
2024-04-25 15:07:13,272 Epoch number 27, batch number 4/10:       batch loss 0.42331263422966003
2024-04-25 15:07:13,794 Epoch number 27, batch number 5/10:       batch loss 0.4943801760673523
2024-04-25 15:07:14,281 Epoch number 27, batch number 6/10:       batch loss 0.4718098044395447
2024-04-25 15:07:14,762 Epoch number 27, batch number 7/10:       batch loss 0.49757641553878784
2024-04-25 15:07:15,247 Epoch number 27, batch number 8/10:       batch loss 0.5325343608856201
2024-04-25 15:07:15,905 Epoch number 27, batch number 9/10:       batch loss 0.9088858962059021
2024-04-25 15:07:17,065 Epoch number 27, batch number 0/2:       batch loss 0.8000266551971436
2024-04-25 15:07:17,783 Epoch number 27, batch number 1/2:       batch loss 0.8939034342765808
2024-04-25 15:07:17,899 Epoch: 28 	Training Loss: 0.054318
2024-04-25 15:07:17,900 Time for epoch 28 : 8 sec
2024-04-25 15:07:17,900 lr for epoch 28 is 0.01000
2024-04-25 15:07:19,481 Epoch number 28, batch number 0/10:       batch loss 0.848368227481842
2024-04-25 15:07:20,310 Epoch number 28, batch number 1/10:       batch loss 0.9229892492294312
2024-04-25 15:07:20,658 Epoch number 28, batch number 2/10:       batch loss 0.05364590883255005
2024-04-25 15:07:21,041 Epoch number 28, batch number 3/10:       batch loss 0.05472729727625847
2024-04-25 15:07:21,378 Epoch number 28, batch number 4/10:       batch loss 0.05892886593937874
2024-04-25 15:07:21,710 Epoch number 28, batch number 5/10:       batch loss 0.055569324642419815
2024-04-25 15:07:22,043 Epoch number 28, batch number 6/10:       batch loss 0.041569486260414124
2024-04-25 15:07:22,372 Epoch number 28, batch number 7/10:       batch loss 0.049805644899606705
2024-04-25 15:07:22,701 Epoch number 28, batch number 8/10:       batch loss 0.0492401160299778
2024-04-25 15:07:23,038 Epoch number 28, batch number 9/10:       batch loss 0.05438029766082764
2024-04-25 15:07:24,218 Epoch number 28, batch number 0/2:       batch loss 0.04686785116791725
2024-04-25 15:07:24,841 Epoch number 28, batch number 1/2:       batch loss 0.049665458500385284
2024-04-25 15:07:24,931 Epoch: 29 	Training Loss: 0.027365
2024-04-25 15:07:24,932 Time for epoch 29 : 7 sec
2024-04-25 15:07:24,932 lr for epoch 29 is 0.01000
2024-04-25 15:07:26,198 Epoch number 29, batch number 0/10:       batch loss 0.06251957267522812
2024-04-25 15:07:26,708 Epoch number 29, batch number 1/10:       batch loss 0.04592511057853699
2024-04-25 15:07:27,136 Epoch number 29, batch number 2/10:       batch loss 0.045655086636543274
2024-04-25 15:07:27,484 Epoch number 29, batch number 3/10:       batch loss 0.04325777664780617
2024-04-25 15:07:27,821 Epoch number 29, batch number 4/10:       batch loss 0.05698143690824509
2024-04-25 15:07:28,164 Epoch number 29, batch number 5/10:       batch loss 0.054220426827669144
2024-04-25 15:07:28,503 Epoch number 29, batch number 6/10:       batch loss 0.052801549434661865
2024-04-25 15:07:28,838 Epoch number 29, batch number 7/10:       batch loss 0.05825485289096832
2024-04-25 15:07:29,169 Epoch number 29, batch number 8/10:       batch loss 0.05553286150097847
2024-04-25 15:07:29,629 Epoch number 29, batch number 9/10:       batch loss 0.5052187442779541
2024-04-25 15:07:30,808 Epoch number 29, batch number 0/2:       batch loss 0.4298919141292572
2024-04-25 15:07:31,479 Epoch number 29, batch number 1/2:       batch loss 0.39937883615493774
2024-04-25 15:07:31,604 Epoch: 30 	Training Loss: 0.012255
2024-04-25 15:07:31,604 Time for epoch 30 : 7 sec
2024-04-25 15:07:31,604 lr for epoch 30 is 0.01000
2024-04-25 15:07:33,084 Epoch number 30, batch number 0/10:       batch loss 0.4672863781452179
2024-04-25 15:07:33,647 Epoch number 30, batch number 1/10:       batch loss 0.4477413594722748
2024-04-25 15:07:34,151 Epoch number 30, batch number 2/10:       batch loss 0.5136188864707947
2024-04-25 15:07:34,689 Epoch number 30, batch number 3/10:       batch loss 0.526589572429657
2024-04-25 15:07:35,171 Epoch number 30, batch number 4/10:       batch loss 0.5799181461334229
2024-04-25 15:07:35,651 Epoch number 30, batch number 5/10:       batch loss 0.5355220437049866
2024-04-25 15:07:36,136 Epoch number 30, batch number 6/10:       batch loss 0.5392552614212036
2024-04-25 15:07:36,610 Epoch number 30, batch number 7/10:       batch loss 0.45487841963768005
2024-04-25 15:07:37,146 Epoch number 30, batch number 8/10:       batch loss 0.5679912567138672
2024-04-25 15:07:37,662 Epoch number 30, batch number 9/10:       batch loss 0.4715231657028198
2024-04-25 15:07:38,874 Epoch number 30, batch number 0/2:       batch loss 0.5278218984603882
2024-04-25 15:07:39,552 Epoch number 30, batch number 1/2:       batch loss 0.5180209875106812
2024-04-25 15:07:39,681 Epoch: 31 	Training Loss: 0.063804
2024-04-25 15:07:39,682 Time for epoch 31 : 8 sec
2024-04-25 15:07:39,682 lr for epoch 31 is 0.01000
2024-04-25 15:07:41,165 Epoch number 31, batch number 0/10:       batch loss 0.547838568687439
2024-04-25 15:07:41,795 Epoch number 31, batch number 1/10:       batch loss 0.6045846939086914
2024-04-25 15:07:42,291 Epoch number 31, batch number 2/10:       batch loss 0.6147043108940125
2024-04-25 15:07:42,823 Epoch number 31, batch number 3/10:       batch loss 0.512320339679718
2024-04-25 15:07:43,339 Epoch number 31, batch number 4/10:       batch loss 0.5988120436668396
2024-04-25 15:07:43,843 Epoch number 31, batch number 5/10:       batch loss 0.5422422885894775
2024-04-25 15:07:44,329 Epoch number 31, batch number 6/10:       batch loss 0.5433192253112793
2024-04-25 15:07:44,810 Epoch number 31, batch number 7/10:       batch loss 0.5977578163146973
2024-04-25 15:07:45,284 Epoch number 31, batch number 8/10:       batch loss 0.5186400413513184
2024-04-25 15:07:45,762 Epoch number 31, batch number 9/10:       batch loss 0.4605872631072998
2024-04-25 15:07:47,176 Epoch number 31, batch number 0/2:       batch loss 0.5132238864898682
2024-04-25 15:07:47,842 Epoch number 31, batch number 1/2:       batch loss 0.49574071168899536
2024-04-25 15:07:47,959 Epoch: 32 	Training Loss: 0.069260
2024-04-25 15:07:47,959 Time for epoch 32 : 8 sec
2024-04-25 15:07:47,960 lr for epoch 32 is 0.01000
2024-04-25 15:07:49,457 Epoch number 32, batch number 0/10:       batch loss 0.5246620178222656
2024-04-25 15:07:50,111 Epoch number 32, batch number 1/10:       batch loss 0.5101509094238281
2024-04-25 15:07:50,609 Epoch number 32, batch number 2/10:       batch loss 0.5364400148391724
2024-04-25 15:07:51,106 Epoch number 32, batch number 3/10:       batch loss 0.4965938627719879
2024-04-25 15:07:51,661 Epoch number 32, batch number 4/10:       batch loss 0.5822831988334656
2024-04-25 15:07:52,152 Epoch number 32, batch number 5/10:       batch loss 0.5990170240402222
2024-04-25 15:07:52,632 Epoch number 32, batch number 6/10:       batch loss 0.565057635307312
2024-04-25 15:07:53,115 Epoch number 32, batch number 7/10:       batch loss 0.5494251847267151
2024-04-25 15:07:53,587 Epoch number 32, batch number 8/10:       batch loss 0.5918946266174316
2024-04-25 15:07:54,068 Epoch number 32, batch number 9/10:       batch loss 0.5376433730125427
2024-04-25 15:07:55,260 Epoch number 32, batch number 0/2:       batch loss 0.5661168098449707
2024-04-25 15:07:55,948 Epoch number 32, batch number 1/2:       batch loss 0.5075879693031311
2024-04-25 15:07:56,039 Epoch: 33 	Training Loss: 0.068665
2024-04-25 15:07:56,039 Time for epoch 33 : 8 sec
2024-04-25 15:07:56,039 lr for epoch 33 is 0.01000
2024-04-25 15:07:57,570 Epoch number 33, batch number 0/10:       batch loss 0.5670156478881836
2024-04-25 15:07:58,191 Epoch number 33, batch number 1/10:       batch loss 0.6413582563400269
2024-04-25 15:07:58,762 Epoch number 33, batch number 2/10:       batch loss 0.48942115902900696
2024-04-25 15:07:59,412 Epoch number 33, batch number 3/10:       batch loss 0.9215347170829773
2024-04-25 15:08:00,048 Epoch number 33, batch number 4/10:       batch loss 0.9239222407341003
2024-04-25 15:08:01,632 Epoch number 33, batch number 5/10:       batch loss 1.7986841201782227
2024-04-25 15:08:03,151 Epoch number 33, batch number 6/10:       batch loss 1.9925700426101685
2024-04-25 15:08:04,609 Epoch number 33, batch number 7/10:       batch loss 1.659218430519104
2024-04-25 15:08:06,075 Epoch number 33, batch number 8/10:       batch loss 1.8493385314941406
2024-04-25 15:08:07,541 Epoch number 33, batch number 9/10:       batch loss 1.8813670873641968
2024-04-25 15:08:08,773 Epoch number 33, batch number 0/2:       batch loss 0.8699815273284912
2024-04-25 15:08:09,493 Epoch number 33, batch number 1/2:       batch loss 0.8667614459991455
2024-04-25 15:08:09,574 Epoch: 34 	Training Loss: 0.159055
2024-04-25 15:08:09,574 Time for epoch 34 : 14 sec
2024-04-25 15:08:09,574 lr for epoch 34 is 0.01000
2024-04-25 15:08:11,252 Epoch number 34, batch number 0/10:       batch loss 0.8627519607543945
2024-04-25 15:08:12,044 Epoch number 34, batch number 1/10:       batch loss 0.879805862903595
2024-04-25 15:08:12,509 Epoch number 34, batch number 2/10:       batch loss 0.3473661541938782
2024-04-25 15:08:12,947 Epoch number 34, batch number 3/10:       batch loss 0.48383828997612
2024-04-25 15:08:13,392 Epoch number 34, batch number 4/10:       batch loss 0.4062809944152832
2024-04-25 15:08:14,011 Epoch number 34, batch number 5/10:       batch loss 0.9410708546638489
2024-04-25 15:08:14,647 Epoch number 34, batch number 6/10:       batch loss 0.9077270030975342
2024-04-25 15:08:15,277 Epoch number 34, batch number 7/10:       batch loss 0.9665584564208984
2024-04-25 15:08:15,904 Epoch number 34, batch number 8/10:       batch loss 0.7549856305122375
2024-04-25 15:08:17,501 Epoch number 34, batch number 9/10:       batch loss 2.3053088188171387
2024-04-25 15:08:19,039 Epoch number 34, batch number 0/2:       batch loss 1.112807273864746
2024-04-25 15:08:19,809 Epoch number 34, batch number 1/2:       batch loss 1.0133202075958252
2024-04-25 15:08:19,919 Epoch: 35 	Training Loss: 0.110696
2024-04-25 15:08:19,920 Time for epoch 35 : 10 sec
2024-04-25 15:08:19,920 lr for epoch 35 is 0.01000
2024-04-25 15:08:21,681 Epoch number 35, batch number 0/10:       batch loss 1.2225819826126099
2024-04-25 15:08:22,727 Epoch number 35, batch number 1/10:       batch loss 1.1113348007202148
2024-04-25 15:08:24,284 Epoch number 35, batch number 2/10:       batch loss 1.7047336101531982
2024-04-25 15:08:25,788 Epoch number 35, batch number 3/10:       batch loss 1.6407458782196045
2024-04-25 15:08:27,274 Epoch number 35, batch number 4/10:       batch loss 1.8698513507843018
2024-04-25 15:08:28,767 Epoch number 35, batch number 5/10:       batch loss 1.5678666830062866
2024-04-25 15:08:29,718 Epoch number 35, batch number 6/10:       batch loss 1.3731186389923096
2024-04-25 15:08:30,666 Epoch number 35, batch number 7/10:       batch loss 1.3571001291275024
2024-04-25 15:08:31,600 Epoch number 35, batch number 8/10:       batch loss 1.4925620555877686
2024-04-25 15:08:32,542 Epoch number 35, batch number 9/10:       batch loss 1.3556596040725708
2024-04-25 15:08:33,879 Epoch number 35, batch number 0/2:       batch loss 1.2883236408233643
2024-04-25 15:08:34,660 Epoch number 35, batch number 1/2:       batch loss 1.319601058959961
2024-04-25 15:08:34,767 Epoch: 36 	Training Loss: 0.183694
2024-04-25 15:08:34,767 Time for epoch 36 : 15 sec
2024-04-25 15:08:34,767 lr for epoch 36 is 0.01000
2024-04-25 15:08:36,748 Epoch number 36, batch number 0/10:       batch loss 1.3985910415649414
2024-04-25 15:08:37,849 Epoch number 36, batch number 1/10:       batch loss 1.265707015991211
2024-04-25 15:08:38,892 Epoch number 36, batch number 2/10:       batch loss 1.291927695274353
2024-04-25 15:08:39,870 Epoch number 36, batch number 3/10:       batch loss 1.044298529624939
2024-04-25 15:08:40,356 Epoch number 36, batch number 4/10:       batch loss 0.575812578201294
2024-04-25 15:08:40,835 Epoch number 36, batch number 5/10:       batch loss 0.5267152190208435
2024-04-25 15:08:41,320 Epoch number 36, batch number 6/10:       batch loss 0.5349826812744141
2024-04-25 15:08:41,794 Epoch number 36, batch number 7/10:       batch loss 0.5176147222518921
2024-04-25 15:08:42,269 Epoch number 36, batch number 8/10:       batch loss 0.47052711248397827
2024-04-25 15:08:42,743 Epoch number 36, batch number 9/10:       batch loss 0.4426920413970947
2024-04-25 15:08:44,020 Epoch number 36, batch number 0/2:       batch loss 0.7935416102409363
2024-04-25 15:08:44,784 Epoch number 36, batch number 1/2:       batch loss 0.7517548203468323
2024-04-25 15:08:44,892 Epoch: 37 	Training Loss: 0.100861
2024-04-25 15:08:44,892 Time for epoch 37 : 10 sec
2024-04-25 15:08:44,892 lr for epoch 37 is 0.01000
2024-04-25 15:08:46,738 Epoch number 37, batch number 0/10:       batch loss 0.7233152985572815
2024-04-25 15:08:47,694 Epoch number 37, batch number 1/10:       batch loss 0.8135899305343628
2024-04-25 15:08:48,561 Epoch number 37, batch number 2/10:       batch loss 0.78803551197052
2024-04-25 15:08:49,104 Epoch number 37, batch number 3/10:       batch loss 0.38009437918663025
2024-04-25 15:08:49,621 Epoch number 37, batch number 4/10:       batch loss 0.46244463324546814
2024-04-25 15:08:50,142 Epoch number 37, batch number 5/10:       batch loss 0.46490395069122314
2024-04-25 15:08:50,673 Epoch number 37, batch number 6/10:       batch loss 0.3420746624469757
2024-04-25 15:08:51,262 Epoch number 37, batch number 7/10:       batch loss 0.4122171103954315
2024-04-25 15:08:51,787 Epoch number 37, batch number 8/10:       batch loss 0.3806121051311493
2024-04-25 15:08:52,311 Epoch number 37, batch number 9/10:       batch loss 0.28733009099960327
2024-04-25 15:08:53,660 Epoch number 37, batch number 0/2:       batch loss 0.25609004497528076
2024-04-25 15:08:54,354 Epoch number 37, batch number 1/2:       batch loss 0.250775009393692
2024-04-25 15:08:54,452 Epoch: 38 	Training Loss: 0.063183
2024-04-25 15:08:54,452 Time for epoch 38 : 10 sec
2024-04-25 15:08:54,452 lr for epoch 38 is 0.01000
2024-04-25 15:08:56,005 Epoch number 38, batch number 0/10:       batch loss 0.26171794533729553
2024-04-25 15:08:56,688 Epoch number 38, batch number 1/10:       batch loss 0.5644146203994751
2024-04-25 15:08:57,237 Epoch number 38, batch number 2/10:       batch loss 0.4526645541191101
2024-04-25 15:08:57,801 Epoch number 38, batch number 3/10:       batch loss 0.43849509954452515
2024-04-25 15:08:58,366 Epoch number 38, batch number 4/10:       batch loss 0.5076507925987244
2024-04-25 15:08:58,854 Epoch number 38, batch number 5/10:       batch loss 0.40275177359580994
2024-04-25 15:08:59,342 Epoch number 38, batch number 6/10:       batch loss 0.4623262584209442
2024-04-25 15:08:59,826 Epoch number 38, batch number 7/10:       batch loss 0.4667986333370209
2024-04-25 15:09:00,306 Epoch number 38, batch number 8/10:       batch loss 0.3889091908931732
2024-04-25 15:09:01,298 Epoch number 38, batch number 9/10:       batch loss 1.3699824810028076
2024-04-25 15:09:02,627 Epoch number 38, batch number 0/2:       batch loss 1.2535600662231445
2024-04-25 15:09:03,422 Epoch number 38, batch number 1/2:       batch loss 1.1189929246902466
2024-04-25 15:09:03,531 Epoch: 39 	Training Loss: 0.066446
2024-04-25 15:09:03,531 Time for epoch 39 : 9 sec
2024-04-25 15:09:03,531 lr for epoch 39 is 0.01000
2024-04-25 15:09:05,575 Epoch number 39, batch number 0/10:       batch loss 1.3469265699386597
2024-04-25 15:09:06,739 Epoch number 39, batch number 1/10:       batch loss 1.3589140176773071
2024-04-25 15:09:07,739 Epoch number 39, batch number 2/10:       batch loss 1.130174160003662
2024-04-25 15:09:08,708 Epoch number 39, batch number 3/10:       batch loss 1.0603477954864502
2024-04-25 15:09:09,198 Epoch number 39, batch number 4/10:       batch loss 0.4666619896888733
2024-04-25 15:09:09,679 Epoch number 39, batch number 5/10:       batch loss 0.4834268391132355
2024-04-25 15:09:10,163 Epoch number 39, batch number 6/10:       batch loss 0.37691810727119446
2024-04-25 15:09:10,679 Epoch number 39, batch number 7/10:       batch loss 0.3901039659976959
2024-04-25 15:09:11,167 Epoch number 39, batch number 8/10:       batch loss 0.48652294278144836
2024-04-25 15:09:11,656 Epoch number 39, batch number 9/10:       batch loss 0.48799899220466614
2024-04-25 15:09:12,799 Epoch number 39, batch number 0/2:       batch loss 0.4589722156524658
2024-04-25 15:09:13,467 Epoch number 39, batch number 1/2:       batch loss 0.46534615755081177
2024-04-25 15:09:13,553 Epoch: 40 	Training Loss: 0.094850
2024-04-25 15:09:13,553 Time for epoch 40 : 10 sec
2024-04-25 15:09:13,553 lr for epoch 40 is 0.01000
2024-04-25 15:09:20,017 Epoch number 0, batch number 0/2:       batch loss 0.49252986907958984
2024-04-25 15:09:21,000 Epoch number 0, batch number 1/2:       batch loss 0.4669145941734314
2024-04-25 15:09:21,022 Epoch: 1 	Training Loss: 0.059965
2024-04-25 15:09:21,022 Time for epoch 1 : 5 sec
2024-04-25 15:09:21,022 lr for epoch 1 is 0.01000
2024-04-25 15:09:21,810 Epoch number 0, batch number 0/2:       batch loss 0.46759334206581116
2024-04-25 15:09:22,485 Epoch number 0, batch number 1/2:       batch loss 0.39721423387527466
2024-04-25 15:09:26,372 Epoch number 1, batch number 0/2:       batch loss 0.4732019305229187
2024-04-25 15:09:27,352 Epoch number 1, batch number 1/2:       batch loss 0.4683727025985718
2024-04-25 15:09:27,370 Epoch: 2 	Training Loss: 0.058848
2024-04-25 15:09:27,370 Time for epoch 2 : 5 sec
2024-04-25 15:09:27,370 lr for epoch 2 is 0.01000
2024-04-25 15:09:28,175 Epoch number 1, batch number 0/2:       batch loss 0.4040002226829529
2024-04-25 15:09:28,790 Epoch number 1, batch number 1/2:       batch loss 0.47029346227645874
2024-04-25 15:09:32,672 Epoch number 2, batch number 0/2:       batch loss 0.45765119791030884
2024-04-25 15:09:33,653 Epoch number 2, batch number 1/2:       batch loss 0.44505003094673157
2024-04-25 15:09:33,669 Epoch: 3 	Training Loss: 0.056419
2024-04-25 15:09:33,669 Time for epoch 3 : 5 sec
2024-04-25 15:09:33,669 lr for epoch 3 is 0.01000
2024-04-25 15:09:34,508 Epoch number 2, batch number 0/2:       batch loss 0.4466058611869812
2024-04-25 15:09:35,186 Epoch number 2, batch number 1/2:       batch loss 0.4088992178440094
2024-04-25 15:09:39,014 Epoch number 3, batch number 0/2:       batch loss 0.4553453326225281
2024-04-25 15:09:39,996 Epoch number 3, batch number 1/2:       batch loss 0.4730618894100189
2024-04-25 15:09:40,016 Epoch: 4 	Training Loss: 0.058025
2024-04-25 15:09:40,016 Time for epoch 4 : 5 sec
2024-04-25 15:09:40,016 lr for epoch 4 is 0.01000
2024-04-25 15:09:40,839 Epoch number 3, batch number 0/2:       batch loss 0.4616532623767853
2024-04-25 15:09:41,470 Epoch number 3, batch number 1/2:       batch loss 0.4371054470539093
2024-04-25 15:09:45,384 Epoch number 4, batch number 0/2:       batch loss 0.45697686076164246
2024-04-25 15:09:46,371 Epoch number 4, batch number 1/2:       batch loss 0.4766635000705719
2024-04-25 15:09:46,390 Epoch: 5 	Training Loss: 0.058353
2024-04-25 15:09:46,391 Time for epoch 5 : 5 sec
2024-04-25 15:09:46,391 lr for epoch 5 is 0.01000
2024-04-25 15:09:47,207 Epoch number 4, batch number 0/2:       batch loss 0.4267788231372833
2024-04-25 15:09:47,911 Epoch number 4, batch number 1/2:       batch loss 0.47807809710502625
2024-04-25 15:09:51,806 Epoch number 5, batch number 0/2:       batch loss 0.46198803186416626
2024-04-25 15:09:52,796 Epoch number 5, batch number 1/2:       batch loss 0.47736936807632446
2024-04-25 15:09:52,816 Epoch: 6 	Training Loss: 0.058710
2024-04-25 15:09:52,816 Time for epoch 6 : 5 sec
2024-04-25 15:09:52,816 lr for epoch 6 is 0.01000
2024-04-25 15:09:53,558 Epoch number 5, batch number 0/2:       batch loss 0.43888425827026367
2024-04-25 15:09:54,276 Epoch number 5, batch number 1/2:       batch loss 0.45170271396636963
2024-04-25 15:09:58,162 Epoch number 6, batch number 0/2:       batch loss 0.4341390132904053
2024-04-25 15:09:59,147 Epoch number 6, batch number 1/2:       batch loss 0.47341859340667725
2024-04-25 15:09:59,166 Epoch: 7 	Training Loss: 0.056722
2024-04-25 15:09:59,166 Time for epoch 7 : 5 sec
2024-04-25 15:09:59,166 lr for epoch 7 is 0.01000
2024-04-25 15:09:59,957 Epoch number 6, batch number 0/2:       batch loss 0.500834584236145
2024-04-25 15:10:00,600 Epoch number 6, batch number 1/2:       batch loss 0.4054775536060333
2024-04-25 15:10:04,507 Epoch number 7, batch number 0/2:       batch loss 0.47937747836112976
2024-04-25 15:10:05,493 Epoch number 7, batch number 1/2:       batch loss 0.4755515158176422
2024-04-25 15:10:05,509 Epoch: 8 	Training Loss: 0.059683
2024-04-25 15:10:05,509 Time for epoch 8 : 5 sec
2024-04-25 15:10:05,509 lr for epoch 8 is 0.01000
2024-04-25 15:10:06,302 Epoch number 7, batch number 0/2:       batch loss 0.5145519971847534
2024-04-25 15:10:07,011 Epoch number 7, batch number 1/2:       batch loss 0.48711857199668884
2024-04-25 15:10:10,905 Epoch number 8, batch number 0/2:       batch loss 0.5100597143173218
2024-04-25 15:10:11,886 Epoch number 8, batch number 1/2:       batch loss 0.48911571502685547
2024-04-25 15:10:11,896 Epoch: 9 	Training Loss: 0.062448
2024-04-25 15:10:11,896 Time for epoch 9 : 5 sec
2024-04-25 15:10:11,896 lr for epoch 9 is 0.01000
2024-04-25 15:10:12,669 Epoch number 8, batch number 0/2:       batch loss 0.4463135004043579
2024-04-25 15:10:13,375 Epoch number 8, batch number 1/2:       batch loss 0.5081555843353271
2024-04-25 15:10:17,284 Epoch number 9, batch number 0/2:       batch loss 0.4804038405418396
2024-04-25 15:10:18,263 Epoch number 9, batch number 1/2:       batch loss 0.47871679067611694
2024-04-25 15:10:18,280 Epoch: 10 	Training Loss: 0.059945
2024-04-25 15:10:18,280 Time for epoch 10 : 5 sec
2024-04-25 15:10:18,280 lr for epoch 10 is 0.01000
2024-04-25 15:10:19,064 Epoch number 9, batch number 0/2:       batch loss 0.42594173550605774
2024-04-25 15:10:19,706 Epoch number 9, batch number 1/2:       batch loss 0.41412413120269775
2024-04-25 15:10:48,051 findfont: Font family 'Arial' not found.
2024-04-25 15:10:48,051 findfont: Font family 'Arial' not found.
2024-04-25 15:10:48,052 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:10:48,052 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:10:48,058 findfont: Font family 'Arial' not found.
2024-04-25 15:10:48,058 findfont: Font family 'Arial' not found.
2024-04-25 15:10:48,064 findfont: Font family 'Arial' not found.
2024-04-25 15:10:48,065 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:11:18,424 findfont: Font family 'Arial' not found.
2024-04-25 15:11:18,424 findfont: Font family 'Arial' not found.
2024-04-25 15:11:18,425 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:11:18,425 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:11:18,431 findfont: Font family 'Arial' not found.
2024-04-25 15:11:18,431 findfont: Font family 'Arial' not found.
2024-04-25 15:11:18,435 findfont: Font family 'Arial' not found.
2024-04-25 15:11:18,436 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:11:49,677 findfont: Font family 'Arial' not found.
2024-04-25 15:11:49,677 findfont: Font family 'Arial' not found.
2024-04-25 15:11:49,677 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:11:49,677 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:11:49,684 findfont: Font family 'Arial' not found.
2024-04-25 15:11:49,684 findfont: Font family 'Arial' not found.
2024-04-25 15:11:49,689 findfont: Font family 'Arial' not found.
2024-04-25 15:11:49,690 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:11:56,907 Run Finished Successfully
