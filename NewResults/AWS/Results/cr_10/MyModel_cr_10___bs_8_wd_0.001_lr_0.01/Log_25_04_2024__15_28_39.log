2024-04-25 15:28:39,248 This is a summery of the run:
2024-04-25 15:28:39,248 Batch size for this run: 8
2024-04-25 15:28:39,249 Size of original image: 32 X 32
2024-04-25 15:28:39,249 number of masks: 102
2024-04-25 15:28:39,249 Compression ratio: 10
2024-04-25 15:28:39,249 epochs : 40
2024-04-25 15:28:39,249 one learning rate: 0.01
2024-04-25 15:28:39,249 optimizer: adam
2024-04-25 15:28:39,249 weight_decay: 0.001
2024-04-25 15:28:39,249 ***************************************************************************


2024-04-25 15:28:39,249 learning rate: 0.01
2024-04-25 15:28:42,824 Epoch number 0, batch number 0/10:       batch loss 1.4508557319641113
2024-04-25 15:28:43,693 Epoch number 0, batch number 1/10:       batch loss 1.0824298858642578
2024-04-25 15:28:44,496 Epoch number 0, batch number 2/10:       batch loss 1.1698426008224487
2024-04-25 15:28:45,315 Epoch number 0, batch number 3/10:       batch loss 1.1175508499145508
2024-04-25 15:28:46,139 Epoch number 0, batch number 4/10:       batch loss 1.104573130607605
2024-04-25 15:28:46,951 Epoch number 0, batch number 5/10:       batch loss 1.1576343774795532
2024-04-25 15:28:47,602 Epoch number 0, batch number 6/10:       batch loss 1.1033250093460083
2024-04-25 15:28:48,738 Epoch number 0, batch number 7/10:       batch loss 1.47837233543396
2024-04-25 15:28:49,838 Epoch number 0, batch number 8/10:       batch loss 2.150346279144287
2024-04-25 15:28:50,934 Epoch number 0, batch number 9/10:       batch loss 1.9401962757110596
2024-04-25 15:28:52,357 Epoch number 0, batch number 0/2:       batch loss 1.6618918180465698
2024-04-25 15:28:53,239 Epoch number 0, batch number 1/2:       batch loss 2.5308780670166016
2024-04-25 15:28:53,342 Epoch: 1 	Training Loss: 0.171939
2024-04-25 15:28:53,342 Time for epoch 1 : 14 sec
2024-04-25 15:28:53,342 lr for epoch 1 is 0.01000
2024-04-25 15:28:55,504 Epoch number 1, batch number 0/10:       batch loss 2.0985753536224365
2024-04-25 15:28:56,863 Epoch number 1, batch number 1/10:       batch loss 2.222261428833008
2024-04-25 15:28:57,996 Epoch number 1, batch number 2/10:       batch loss 2.4431939125061035
2024-04-25 15:28:59,086 Epoch number 1, batch number 3/10:       batch loss 2.4777116775512695
2024-04-25 15:29:00,164 Epoch number 1, batch number 4/10:       batch loss 2.995148181915283
2024-04-25 15:29:01,234 Epoch number 1, batch number 5/10:       batch loss 3.7278785705566406
2024-04-25 15:29:02,314 Epoch number 1, batch number 6/10:       batch loss 3.8970870971679688
2024-04-25 15:29:03,397 Epoch number 1, batch number 7/10:       batch loss 3.35427188873291
2024-04-25 15:29:04,469 Epoch number 1, batch number 8/10:       batch loss 3.2687532901763916
2024-04-25 15:29:05,539 Epoch number 1, batch number 9/10:       batch loss 2.2097535133361816
2024-04-25 15:29:06,907 Epoch number 1, batch number 0/2:       batch loss 2.1734495162963867
2024-04-25 15:29:07,789 Epoch number 1, batch number 1/2:       batch loss 2.1131062507629395
2024-04-25 15:29:07,894 Epoch: 2 	Training Loss: 0.358683
2024-04-25 15:29:07,895 Time for epoch 2 : 15 sec
2024-04-25 15:29:07,895 lr for epoch 2 is 0.01000
2024-04-25 15:29:09,977 Epoch number 2, batch number 0/10:       batch loss 2.289335012435913
2024-04-25 15:29:11,256 Epoch number 2, batch number 1/10:       batch loss 1.7245821952819824
2024-04-25 15:29:12,523 Epoch number 2, batch number 2/10:       batch loss 2.0223398208618164
2024-04-25 15:29:13,623 Epoch number 2, batch number 3/10:       batch loss 1.7274024486541748
2024-04-25 15:29:14,701 Epoch number 2, batch number 4/10:       batch loss 1.8559507131576538
2024-04-25 15:29:15,774 Epoch number 2, batch number 5/10:       batch loss 1.8080458641052246
2024-04-25 15:29:16,849 Epoch number 2, batch number 6/10:       batch loss 1.975340723991394
2024-04-25 15:29:17,922 Epoch number 2, batch number 7/10:       batch loss 2.0036513805389404
2024-04-25 15:29:18,990 Epoch number 2, batch number 8/10:       batch loss 2.5541296005249023
2024-04-25 15:29:20,062 Epoch number 2, batch number 9/10:       batch loss 2.190474033355713
2024-04-25 15:29:21,446 Epoch number 2, batch number 0/2:       batch loss 1.9267158508300781
2024-04-25 15:29:22,327 Epoch number 2, batch number 1/2:       batch loss 1.9407806396484375
2024-04-25 15:29:22,443 Epoch: 3 	Training Loss: 0.251891
2024-04-25 15:29:22,443 Time for epoch 3 : 15 sec
2024-04-25 15:29:22,443 lr for epoch 3 is 0.01000
2024-04-25 15:29:24,552 Epoch number 3, batch number 0/10:       batch loss 1.6725090742111206
2024-04-25 15:29:25,850 Epoch number 3, batch number 1/10:       batch loss 2.1948184967041016
2024-04-25 15:29:26,983 Epoch number 3, batch number 2/10:       batch loss 1.898363471031189
2024-04-25 15:29:28,065 Epoch number 3, batch number 3/10:       batch loss 2.153656482696533
2024-04-25 15:29:29,148 Epoch number 3, batch number 4/10:       batch loss 2.369910717010498
2024-04-25 15:29:30,241 Epoch number 3, batch number 5/10:       batch loss 2.585041046142578
2024-04-25 15:29:31,308 Epoch number 3, batch number 6/10:       batch loss 2.039450168609619
2024-04-25 15:29:32,385 Epoch number 3, batch number 7/10:       batch loss 2.3615448474884033
2024-04-25 15:29:33,487 Epoch number 3, batch number 8/10:       batch loss 1.9182506799697876
2024-04-25 15:29:34,625 Epoch number 3, batch number 9/10:       batch loss 2.2018918991088867
2024-04-25 15:29:36,006 Epoch number 3, batch number 0/2:       batch loss 1.9512425661087036
2024-04-25 15:29:36,871 Epoch number 3, batch number 1/2:       batch loss 1.8355828523635864
2024-04-25 15:29:36,977 Epoch: 4 	Training Loss: 0.267443
2024-04-25 15:29:36,978 Time for epoch 4 : 15 sec
2024-04-25 15:29:36,978 lr for epoch 4 is 0.01000
2024-04-25 15:29:39,097 Epoch number 4, batch number 0/10:       batch loss 2.131269931793213
2024-04-25 15:29:40,337 Epoch number 4, batch number 1/10:       batch loss 1.9981491565704346
2024-04-25 15:29:41,439 Epoch number 4, batch number 2/10:       batch loss 1.776918888092041
2024-04-25 15:29:42,510 Epoch number 4, batch number 3/10:       batch loss 1.6208794116973877
2024-04-25 15:29:43,637 Epoch number 4, batch number 4/10:       batch loss 1.5511289834976196
2024-04-25 15:29:44,772 Epoch number 4, batch number 5/10:       batch loss 1.911395788192749
2024-04-25 15:29:45,836 Epoch number 4, batch number 6/10:       batch loss 1.9355548620224
2024-04-25 15:29:46,901 Epoch number 4, batch number 7/10:       batch loss 2.35469126701355
2024-04-25 15:29:47,964 Epoch number 4, batch number 8/10:       batch loss 1.9218989610671997
2024-04-25 15:29:49,043 Epoch number 4, batch number 9/10:       batch loss 1.6828652620315552
2024-04-25 15:29:50,404 Epoch number 4, batch number 0/2:       batch loss 2.2375969886779785
2024-04-25 15:29:51,270 Epoch number 4, batch number 1/2:       batch loss 1.8306611776351929
2024-04-25 15:29:51,365 Epoch: 5 	Training Loss: 0.236059
2024-04-25 15:29:51,365 Time for epoch 5 : 14 sec
2024-04-25 15:29:51,365 lr for epoch 5 is 0.01000
2024-04-25 15:29:53,681 Epoch number 5, batch number 0/10:       batch loss 2.0458765029907227
2024-04-25 15:29:54,914 Epoch number 5, batch number 1/10:       batch loss 1.6294950246810913
2024-04-25 15:29:56,030 Epoch number 5, batch number 2/10:       batch loss 1.6094084978103638
2024-04-25 15:29:57,102 Epoch number 5, batch number 3/10:       batch loss 1.6539638042449951
2024-04-25 15:29:58,226 Epoch number 5, batch number 4/10:       batch loss 1.5484490394592285
2024-04-25 15:29:59,308 Epoch number 5, batch number 5/10:       batch loss 1.6905593872070312
2024-04-25 15:30:00,373 Epoch number 5, batch number 6/10:       batch loss 1.6230037212371826
2024-04-25 15:30:01,444 Epoch number 5, batch number 7/10:       batch loss 1.7343859672546387
2024-04-25 15:30:02,521 Epoch number 5, batch number 8/10:       batch loss 1.4709070920944214
2024-04-25 15:30:03,586 Epoch number 5, batch number 9/10:       batch loss 1.5385864973068237
2024-04-25 15:30:04,960 Epoch number 5, batch number 0/2:       batch loss 1.5853400230407715
2024-04-25 15:30:05,833 Epoch number 5, batch number 1/2:       batch loss 1.5160572528839111
2024-04-25 15:30:05,922 Epoch: 6 	Training Loss: 0.206808
2024-04-25 15:30:05,922 Time for epoch 6 : 15 sec
2024-04-25 15:30:05,922 lr for epoch 6 is 0.01000
2024-04-25 15:30:07,983 Epoch number 6, batch number 0/10:       batch loss 1.6698901653289795
2024-04-25 15:30:09,256 Epoch number 6, batch number 1/10:       batch loss 1.6352794170379639
2024-04-25 15:30:10,412 Epoch number 6, batch number 2/10:       batch loss 1.4132558107376099
2024-04-25 15:30:11,459 Epoch number 6, batch number 3/10:       batch loss 1.6238597631454468
2024-04-25 15:30:12,518 Epoch number 6, batch number 4/10:       batch loss 1.7222075462341309
2024-04-25 15:30:13,560 Epoch number 6, batch number 5/10:       batch loss 1.7596310377120972
2024-04-25 15:30:14,631 Epoch number 6, batch number 6/10:       batch loss 1.689687967300415
2024-04-25 15:30:15,689 Epoch number 6, batch number 7/10:       batch loss 1.9243981838226318
2024-04-25 15:30:16,740 Epoch number 6, batch number 8/10:       batch loss 1.7346713542938232
2024-04-25 15:30:17,796 Epoch number 6, batch number 9/10:       batch loss 1.5797863006591797
2024-04-25 15:30:19,266 Epoch number 6, batch number 0/2:       batch loss 1.8003472089767456
2024-04-25 15:30:20,135 Epoch number 6, batch number 1/2:       batch loss 1.8188233375549316
2024-04-25 15:30:20,234 Epoch: 7 	Training Loss: 0.209408
2024-04-25 15:30:20,234 Time for epoch 7 : 14 sec
2024-04-25 15:30:20,234 lr for epoch 7 is 0.01000
2024-04-25 15:30:22,372 Epoch number 7, batch number 0/10:       batch loss 1.8538939952850342
2024-04-25 15:30:23,188 Epoch number 7, batch number 1/10:       batch loss 1.297269582748413
2024-04-25 15:30:23,903 Epoch number 7, batch number 2/10:       batch loss 1.258934736251831
2024-04-25 15:30:24,534 Epoch number 7, batch number 3/10:       batch loss 1.163291573524475
2024-04-25 15:30:25,164 Epoch number 7, batch number 4/10:       batch loss 0.8831865787506104
2024-04-25 15:30:26,650 Epoch number 7, batch number 5/10:       batch loss 1.8217527866363525
2024-04-25 15:30:28,110 Epoch number 7, batch number 6/10:       batch loss 1.402800440788269
2024-04-25 15:30:29,513 Epoch number 7, batch number 7/10:       batch loss 1.1765153408050537
2024-04-25 15:30:30,922 Epoch number 7, batch number 8/10:       batch loss 1.150905728340149
2024-04-25 15:30:32,330 Epoch number 7, batch number 9/10:       batch loss 1.140946865081787
2024-04-25 15:30:33,853 Epoch number 7, batch number 0/2:       batch loss 1.2427213191986084
2024-04-25 15:30:34,803 Epoch number 7, batch number 1/2:       batch loss 1.1126248836517334
2024-04-25 15:30:34,908 Epoch: 8 	Training Loss: 0.164369
2024-04-25 15:30:34,908 Time for epoch 8 : 15 sec
2024-04-25 15:30:34,908 lr for epoch 8 is 0.01000
2024-04-25 15:30:37,447 Epoch number 8, batch number 0/10:       batch loss 1.1565637588500977
2024-04-25 15:30:39,092 Epoch number 8, batch number 1/10:       batch loss 1.1536297798156738
2024-04-25 15:30:39,747 Epoch number 8, batch number 2/10:       batch loss 0.8619614839553833
2024-04-25 15:30:40,388 Epoch number 8, batch number 3/10:       batch loss 0.9085928201675415
2024-04-25 15:30:41,029 Epoch number 8, batch number 4/10:       batch loss 0.8935374021530151
2024-04-25 15:30:41,668 Epoch number 8, batch number 5/10:       batch loss 1.127909541130066
2024-04-25 15:30:43,097 Epoch number 8, batch number 6/10:       batch loss 1.7664854526519775
2024-04-25 15:30:44,559 Epoch number 8, batch number 7/10:       batch loss 1.9085605144500732
2024-04-25 15:30:45,986 Epoch number 8, batch number 8/10:       batch loss 1.950385570526123
2024-04-25 15:30:47,399 Epoch number 8, batch number 9/10:       batch loss 1.8973766565322876
2024-04-25 15:30:48,909 Epoch number 8, batch number 0/2:       batch loss 1.7670494318008423
2024-04-25 15:30:49,868 Epoch number 8, batch number 1/2:       batch loss 2.3767359256744385
2024-04-25 15:30:49,954 Epoch: 9 	Training Loss: 0.170313
2024-04-25 15:30:49,954 Time for epoch 9 : 15 sec
2024-04-25 15:30:49,954 lr for epoch 9 is 0.01000
2024-04-25 15:30:52,438 Epoch number 9, batch number 0/10:       batch loss 2.2257018089294434
2024-04-25 15:30:53,909 Epoch number 9, batch number 1/10:       batch loss 1.66167414188385
2024-04-25 15:30:55,258 Epoch number 9, batch number 2/10:       batch loss 1.7637360095977783
2024-04-25 15:30:56,760 Epoch number 9, batch number 3/10:       batch loss 2.107844591140747
2024-04-25 15:30:58,172 Epoch number 9, batch number 4/10:       batch loss 1.7646071910858154
2024-04-25 15:30:59,565 Epoch number 9, batch number 5/10:       batch loss 1.412381649017334
2024-04-25 15:31:00,989 Epoch number 9, batch number 6/10:       batch loss 1.3568079471588135
2024-04-25 15:31:02,399 Epoch number 9, batch number 7/10:       batch loss 1.3338674306869507
2024-04-25 15:31:03,808 Epoch number 9, batch number 8/10:       batch loss 1.1386444568634033
2024-04-25 15:31:04,281 Epoch number 9, batch number 9/10:       batch loss 0.057148199528455734
2024-04-25 15:31:05,497 Epoch number 9, batch number 0/2:       batch loss 0.046628888696432114
2024-04-25 15:31:06,184 Epoch number 9, batch number 1/2:       batch loss 0.055665019899606705
2024-04-25 15:31:06,303 Epoch: 10 	Training Loss: 0.185280
2024-04-25 15:31:06,303 Time for epoch 10 : 16 sec
2024-04-25 15:31:06,303 lr for epoch 10 is 0.01000
2024-04-25 15:31:07,716 Epoch number 10, batch number 0/10:       batch loss 0.05288400128483772
2024-04-25 15:31:08,332 Epoch number 10, batch number 1/10:       batch loss 0.058858226984739304
2024-04-25 15:31:08,815 Epoch number 10, batch number 2/10:       batch loss 0.047427646815776825
2024-04-25 15:31:09,237 Epoch number 10, batch number 3/10:       batch loss 0.05605749785900116
2024-04-25 15:31:09,661 Epoch number 10, batch number 4/10:       batch loss 0.05583058297634125
2024-04-25 15:31:10,095 Epoch number 10, batch number 5/10:       batch loss 0.044983215630054474
2024-04-25 15:31:10,517 Epoch number 10, batch number 6/10:       batch loss 0.05001725256443024
2024-04-25 15:31:10,943 Epoch number 10, batch number 7/10:       batch loss 0.04594264551997185
2024-04-25 15:31:11,372 Epoch number 10, batch number 8/10:       batch loss 0.044962357729673386
2024-04-25 15:31:11,797 Epoch number 10, batch number 9/10:       batch loss 0.041593026369810104
2024-04-25 15:31:13,022 Epoch number 10, batch number 0/2:       batch loss 0.04085563123226166
2024-04-25 15:31:13,705 Epoch number 10, batch number 1/2:       batch loss 0.040538959205150604
2024-04-25 15:31:13,816 Epoch: 11 	Training Loss: 0.006232
2024-04-25 15:31:13,816 Time for epoch 11 : 8 sec
2024-04-25 15:31:13,816 lr for epoch 11 is 0.01000
2024-04-25 15:31:15,196 Epoch number 11, batch number 0/10:       batch loss 0.03711841255426407
2024-04-25 15:31:15,761 Epoch number 11, batch number 1/10:       batch loss 0.04367511719465256
2024-04-25 15:31:16,208 Epoch number 11, batch number 2/10:       batch loss 0.044769831001758575
2024-04-25 15:31:16,640 Epoch number 11, batch number 3/10:       batch loss 0.03910548612475395
2024-04-25 15:31:17,061 Epoch number 11, batch number 4/10:       batch loss 0.03582276031374931
2024-04-25 15:31:17,478 Epoch number 11, batch number 5/10:       batch loss 0.033134616911411285
2024-04-25 15:31:17,901 Epoch number 11, batch number 6/10:       batch loss 0.038000598549842834
2024-04-25 15:31:18,337 Epoch number 11, batch number 7/10:       batch loss 0.03109208680689335
2024-04-25 15:31:18,770 Epoch number 11, batch number 8/10:       batch loss 0.03499947115778923
2024-04-25 15:31:19,271 Epoch number 11, batch number 9/10:       batch loss 0.03252207860350609
2024-04-25 15:31:20,523 Epoch number 11, batch number 0/2:       batch loss 0.02844860404729843
2024-04-25 15:31:21,213 Epoch number 11, batch number 1/2:       batch loss 0.03525977581739426
2024-04-25 15:31:21,305 Epoch: 12 	Training Loss: 0.004628
2024-04-25 15:31:21,305 Time for epoch 12 : 7 sec
2024-04-25 15:31:21,305 lr for epoch 12 is 0.01000
2024-04-25 15:31:22,641 Epoch number 12, batch number 0/10:       batch loss 0.0328855887055397
2024-04-25 15:31:23,198 Epoch number 12, batch number 1/10:       batch loss 0.02679017372429371
2024-04-25 15:31:23,631 Epoch number 12, batch number 2/10:       batch loss 0.029533928260207176
2024-04-25 15:31:24,112 Epoch number 12, batch number 3/10:       batch loss 0.037274446338415146
2024-04-25 15:31:24,540 Epoch number 12, batch number 4/10:       batch loss 0.0338158905506134
2024-04-25 15:31:26,108 Epoch number 12, batch number 5/10:       batch loss 0.37217259407043457
2024-04-25 15:31:26,903 Epoch number 12, batch number 6/10:       batch loss 0.6574842929840088
2024-04-25 15:31:27,368 Epoch number 12, batch number 7/10:       batch loss 0.0446951761841774
2024-04-25 15:31:28,149 Epoch number 12, batch number 8/10:       batch loss 0.1890435814857483
2024-04-25 15:31:28,620 Epoch number 12, batch number 9/10:       batch loss 0.05322689935564995
2024-04-25 15:31:30,053 Epoch number 12, batch number 0/2:       batch loss 0.8079488277435303
2024-04-25 15:31:31,019 Epoch number 12, batch number 1/2:       batch loss 0.7522584199905396
2024-04-25 15:31:31,133 Epoch: 13 	Training Loss: 0.018462
2024-04-25 15:31:31,133 Time for epoch 13 : 10 sec
2024-04-25 15:31:31,133 lr for epoch 13 is 0.01000
2024-04-25 15:31:32,990 Epoch number 13, batch number 0/10:       batch loss 0.3091941177845001
2024-04-25 15:31:34,289 Epoch number 13, batch number 1/10:       batch loss 1.2579622268676758
2024-04-25 15:31:35,433 Epoch number 13, batch number 2/10:       batch loss 1.231673240661621
2024-04-25 15:31:36,498 Epoch number 13, batch number 3/10:       batch loss 1.2839759588241577
2024-04-25 15:31:37,954 Epoch number 13, batch number 4/10:       batch loss 1.3009569644927979
2024-04-25 15:31:39,365 Epoch number 13, batch number 5/10:       batch loss 1.219956398010254
2024-04-25 15:31:40,778 Epoch number 13, batch number 6/10:       batch loss 0.7996270656585693
2024-04-25 15:31:41,421 Epoch number 13, batch number 7/10:       batch loss 0.6390609741210938
2024-04-25 15:31:42,137 Epoch number 13, batch number 8/10:       batch loss 0.5677937269210815
2024-04-25 15:31:42,766 Epoch number 13, batch number 9/10:       batch loss 0.6171789169311523
2024-04-25 15:31:44,011 Epoch number 13, batch number 0/2:       batch loss 0.5380032062530518
2024-04-25 15:31:44,766 Epoch number 13, batch number 1/2:       batch loss 0.5767385959625244
2024-04-25 15:31:44,868 Epoch: 14 	Training Loss: 0.115342
2024-04-25 15:31:44,868 Time for epoch 14 : 14 sec
2024-04-25 15:31:44,868 lr for epoch 14 is 0.01000
2024-04-25 15:31:46,453 Epoch number 14, batch number 0/10:       batch loss 0.5945419669151306
2024-04-25 15:31:47,227 Epoch number 14, batch number 1/10:       batch loss 0.5176308751106262
2024-04-25 15:31:47,899 Epoch number 14, batch number 2/10:       batch loss 0.5984726548194885
2024-04-25 15:31:48,546 Epoch number 14, batch number 3/10:       batch loss 0.4882524013519287
2024-04-25 15:31:50,112 Epoch number 14, batch number 4/10:       batch loss 0.7265852689743042
2024-04-25 15:31:51,604 Epoch number 14, batch number 5/10:       batch loss 0.844717264175415
2024-04-25 15:31:52,992 Epoch number 14, batch number 6/10:       batch loss 0.8645239472389221
2024-04-25 15:31:54,380 Epoch number 14, batch number 7/10:       batch loss 0.851976752281189
2024-04-25 15:31:55,764 Epoch number 14, batch number 8/10:       batch loss 0.8548184633255005
2024-04-25 15:31:57,161 Epoch number 14, batch number 9/10:       batch loss 0.8042597770690918
2024-04-25 15:31:58,632 Epoch number 14, batch number 0/2:       batch loss 0.8379576206207275
2024-04-25 15:31:59,576 Epoch number 14, batch number 1/2:       batch loss 0.8424452543258667
2024-04-25 15:31:59,672 Epoch: 15 	Training Loss: 0.089322
2024-04-25 15:31:59,672 Time for epoch 15 : 15 sec
2024-04-25 15:31:59,672 lr for epoch 15 is 0.01000
2024-04-25 15:32:02,162 Epoch number 15, batch number 0/10:       batch loss 0.7414629459381104
2024-04-25 15:32:03,805 Epoch number 15, batch number 1/10:       batch loss 0.7448285818099976
2024-04-25 15:32:05,243 Epoch number 15, batch number 2/10:       batch loss 0.8037742972373962
2024-04-25 15:32:06,696 Epoch number 15, batch number 3/10:       batch loss 0.8946937918663025
2024-04-25 15:32:08,160 Epoch number 15, batch number 4/10:       batch loss 0.8044723868370056
2024-04-25 15:32:09,565 Epoch number 15, batch number 5/10:       batch loss 1.0262210369110107
2024-04-25 15:32:10,981 Epoch number 15, batch number 6/10:       batch loss 0.9548937082290649
2024-04-25 15:32:12,393 Epoch number 15, batch number 7/10:       batch loss 1.066846251487732
2024-04-25 15:32:14,071 Epoch number 15, batch number 8/10:       batch loss 0.9081466197967529
2024-04-25 15:32:15,484 Epoch number 15, batch number 9/10:       batch loss 1.1907634735107422
2024-04-25 15:32:16,974 Epoch number 15, batch number 0/2:       batch loss 0.9008485078811646
2024-04-25 15:32:17,932 Epoch number 15, batch number 1/2:       batch loss 1.1039378643035889
2024-04-25 15:32:18,058 Epoch: 16 	Training Loss: 0.114201
2024-04-25 15:32:18,058 Time for epoch 16 : 18 sec
2024-04-25 15:32:18,058 lr for epoch 16 is 0.01000
2024-04-25 15:32:20,659 Epoch number 16, batch number 0/10:       batch loss 1.0010895729064941
2024-04-25 15:32:22,379 Epoch number 16, batch number 1/10:       batch loss 1.1805468797683716
2024-04-25 15:32:23,849 Epoch number 16, batch number 2/10:       batch loss 0.9250873923301697
2024-04-25 15:32:25,242 Epoch number 16, batch number 3/10:       batch loss 1.0309724807739258
2024-04-25 15:32:26,631 Epoch number 16, batch number 4/10:       batch loss 1.186583399772644
2024-04-25 15:32:28,045 Epoch number 16, batch number 5/10:       batch loss 1.249537467956543
2024-04-25 15:32:29,451 Epoch number 16, batch number 6/10:       batch loss 1.1218583583831787
2024-04-25 15:32:30,860 Epoch number 16, batch number 7/10:       batch loss 1.205700397491455
2024-04-25 15:32:32,273 Epoch number 16, batch number 8/10:       batch loss 1.1316089630126953
2024-04-25 15:32:33,686 Epoch number 16, batch number 9/10:       batch loss 1.37009859085083
2024-04-25 15:32:35,166 Epoch number 16, batch number 0/2:       batch loss 1.1340198516845703
2024-04-25 15:32:36,121 Epoch number 16, batch number 1/2:       batch loss 1.3444112539291382
2024-04-25 15:32:36,224 Epoch: 17 	Training Loss: 0.142539
2024-04-25 15:32:36,224 Time for epoch 17 : 18 sec
2024-04-25 15:32:36,224 lr for epoch 17 is 0.01000
2024-04-25 15:32:38,721 Epoch number 17, batch number 0/10:       batch loss 1.2271201610565186
2024-04-25 15:32:40,431 Epoch number 17, batch number 1/10:       batch loss 1.2598447799682617
2024-04-25 15:32:41,891 Epoch number 17, batch number 2/10:       batch loss 1.3986444473266602
2024-04-25 15:32:43,326 Epoch number 17, batch number 3/10:       batch loss 1.6278835535049438
2024-04-25 15:32:44,751 Epoch number 17, batch number 4/10:       batch loss 1.7741115093231201
2024-04-25 15:32:46,194 Epoch number 17, batch number 5/10:       batch loss 1.519417643547058
2024-04-25 15:32:47,625 Epoch number 17, batch number 6/10:       batch loss 1.6416922807693481
2024-04-25 15:32:49,045 Epoch number 17, batch number 7/10:       batch loss 1.529289960861206
2024-04-25 15:32:50,466 Epoch number 17, batch number 8/10:       batch loss 1.7221741676330566
2024-04-25 15:32:51,898 Epoch number 17, batch number 9/10:       batch loss 1.7878729104995728
2024-04-25 15:32:53,348 Epoch number 17, batch number 0/2:       batch loss 1.9169577360153198
2024-04-25 15:32:54,306 Epoch number 17, batch number 1/2:       batch loss 1.6136046648025513
2024-04-25 15:32:54,415 Epoch: 18 	Training Loss: 0.193601
2024-04-25 15:32:54,415 Time for epoch 18 : 18 sec
2024-04-25 15:32:54,415 lr for epoch 18 is 0.01000
2024-04-25 15:32:56,883 Epoch number 18, batch number 0/10:       batch loss 1.4710898399353027
2024-04-25 15:32:58,397 Epoch number 18, batch number 1/10:       batch loss 1.7476344108581543
2024-04-25 15:32:59,884 Epoch number 18, batch number 2/10:       batch loss 1.9363915920257568
2024-04-25 15:33:01,320 Epoch number 18, batch number 3/10:       batch loss 1.974907636642456
2024-04-25 15:33:02,777 Epoch number 18, batch number 4/10:       batch loss 1.8692470788955688
2024-04-25 15:33:04,220 Epoch number 18, batch number 5/10:       batch loss 1.785050392150879
2024-04-25 15:33:06,168 Epoch number 18, batch number 6/10:       batch loss 1.7612206935882568
2024-04-25 15:33:08,126 Epoch number 18, batch number 7/10:       batch loss 2.2519822120666504
2024-04-25 15:33:10,042 Epoch number 18, batch number 8/10:       batch loss 2.2652435302734375
2024-04-25 15:33:11,959 Epoch number 18, batch number 9/10:       batch loss 2.334522008895874
2024-04-25 15:33:13,663 Epoch number 18, batch number 0/2:       batch loss 2.214552640914917
2024-04-25 15:33:14,761 Epoch number 18, batch number 1/2:       batch loss 2.611340284347534
2024-04-25 15:33:14,871 Epoch: 19 	Training Loss: 0.242466
2024-04-25 15:33:14,872 Time for epoch 19 : 20 sec
2024-04-25 15:33:14,872 lr for epoch 19 is 0.01000
2024-04-25 15:33:17,957 Epoch number 19, batch number 0/10:       batch loss 2.3720743656158447
2024-04-25 15:33:20,103 Epoch number 19, batch number 1/10:       batch loss 2.498208999633789
2024-04-25 15:33:22,138 Epoch number 19, batch number 2/10:       batch loss 2.3167052268981934
2024-04-25 15:33:24,365 Epoch number 19, batch number 3/10:       batch loss 2.643836498260498
2024-04-25 15:33:26,371 Epoch number 19, batch number 4/10:       batch loss 2.278073787689209
2024-04-25 15:33:28,376 Epoch number 19, batch number 5/10:       batch loss 2.288639545440674
2024-04-25 15:33:30,356 Epoch number 19, batch number 6/10:       batch loss 2.3550565242767334
2024-04-25 15:33:32,334 Epoch number 19, batch number 7/10:       batch loss 2.3080458641052246
2024-04-25 15:33:34,289 Epoch number 19, batch number 8/10:       batch loss 1.8418360948562622
2024-04-25 15:33:36,253 Epoch number 19, batch number 9/10:       batch loss 2.2596914768218994
2024-04-25 15:33:37,901 Epoch number 19, batch number 0/2:       batch loss 2.2975926399230957
2024-04-25 15:33:38,984 Epoch number 19, batch number 1/2:       batch loss 2.260298252105713
2024-04-25 15:33:39,064 Epoch: 20 	Training Loss: 0.289527
2024-04-25 15:33:39,064 Time for epoch 20 : 24 sec
2024-04-25 15:33:39,064 lr for epoch 20 is 0.01000
2024-04-25 15:33:42,175 Epoch number 20, batch number 0/10:       batch loss 2.3209962844848633
2024-04-25 15:33:44,360 Epoch number 20, batch number 1/10:       batch loss 2.343322992324829
2024-04-25 15:33:46,401 Epoch number 20, batch number 2/10:       batch loss 2.331934690475464
2024-04-25 15:33:48,426 Epoch number 20, batch number 3/10:       batch loss 2.258155345916748
2024-04-25 15:33:49,066 Epoch number 20, batch number 4/10:       batch loss 0.9350711703300476
2024-04-25 15:33:49,715 Epoch number 20, batch number 5/10:       batch loss 0.9752637147903442
2024-04-25 15:33:50,345 Epoch number 20, batch number 6/10:       batch loss 0.9154361486434937
2024-04-25 15:33:50,989 Epoch number 20, batch number 7/10:       batch loss 0.9665396809577942
2024-04-25 15:33:51,618 Epoch number 20, batch number 8/10:       batch loss 0.9308995008468628
2024-04-25 15:33:52,244 Epoch number 20, batch number 9/10:       batch loss 0.8976512551307678
2024-04-25 15:33:53,536 Epoch number 20, batch number 0/2:       batch loss 0.9454353451728821
2024-04-25 15:33:54,292 Epoch number 20, batch number 1/2:       batch loss 0.8169499635696411
2024-04-25 15:33:54,407 Epoch: 21 	Training Loss: 0.185941
2024-04-25 15:33:54,407 Time for epoch 21 : 15 sec
2024-04-25 15:33:54,407 lr for epoch 21 is 0.01000
2024-04-25 15:33:55,986 Epoch number 21, batch number 0/10:       batch loss 0.8811832070350647
2024-04-25 15:33:56,800 Epoch number 21, batch number 1/10:       batch loss 0.9116094708442688
2024-04-25 15:33:57,468 Epoch number 21, batch number 2/10:       batch loss 0.8978457450866699
2024-04-25 15:33:58,137 Epoch number 21, batch number 3/10:       batch loss 0.6804957985877991
2024-04-25 15:33:58,788 Epoch number 21, batch number 4/10:       batch loss 0.782755434513092
2024-04-25 15:33:59,895 Epoch number 21, batch number 5/10:       batch loss 1.3657283782958984
2024-04-25 15:34:00,991 Epoch number 21, batch number 6/10:       batch loss 0.9851467609405518
2024-04-25 15:34:02,075 Epoch number 21, batch number 7/10:       batch loss 0.9319332838058472
2024-04-25 15:34:03,141 Epoch number 21, batch number 8/10:       batch loss 0.7605300545692444
2024-04-25 15:34:04,208 Epoch number 21, batch number 9/10:       batch loss 0.9121257066726685
2024-04-25 15:34:05,551 Epoch number 21, batch number 0/2:       batch loss 0.8200116753578186
2024-04-25 15:34:06,425 Epoch number 21, batch number 1/2:       batch loss 0.916801929473877
2024-04-25 15:34:06,537 Epoch: 22 	Training Loss: 0.113867
2024-04-25 15:34:06,538 Time for epoch 22 : 12 sec
2024-04-25 15:34:06,538 lr for epoch 22 is 0.01000
2024-04-25 15:34:08,644 Epoch number 22, batch number 0/10:       batch loss 0.8610664010047913
2024-04-25 15:34:09,867 Epoch number 22, batch number 1/10:       batch loss 0.7857894897460938
2024-04-25 15:34:10,968 Epoch number 22, batch number 2/10:       batch loss 0.9195501208305359
2024-04-25 15:34:11,613 Epoch number 22, batch number 3/10:       batch loss 0.6599667072296143
2024-04-25 15:34:12,282 Epoch number 22, batch number 4/10:       batch loss 0.5974093079566956
2024-04-25 15:34:12,909 Epoch number 22, batch number 5/10:       batch loss 0.575332760810852
2024-04-25 15:34:13,538 Epoch number 22, batch number 6/10:       batch loss 0.5555418133735657
2024-04-25 15:34:14,180 Epoch number 22, batch number 7/10:       batch loss 0.5677621364593506
2024-04-25 15:34:14,815 Epoch number 22, batch number 8/10:       batch loss 0.49396443367004395
2024-04-25 15:34:15,449 Epoch number 22, batch number 9/10:       batch loss 0.6104072332382202
2024-04-25 15:34:16,695 Epoch number 22, batch number 0/2:       batch loss 0.6125865578651428
2024-04-25 15:34:17,449 Epoch number 22, batch number 1/2:       batch loss 0.6110449433326721
2024-04-25 15:34:17,566 Epoch: 23 	Training Loss: 0.082835
2024-04-25 15:34:17,566 Time for epoch 23 : 11 sec
2024-04-25 15:34:17,566 lr for epoch 23 is 0.01000
2024-04-25 15:34:19,228 Epoch number 23, batch number 0/10:       batch loss 0.6380622982978821
2024-04-25 15:34:20,044 Epoch number 23, batch number 1/10:       batch loss 0.6759728193283081
2024-04-25 15:34:20,702 Epoch number 23, batch number 2/10:       batch loss 0.6706423759460449
2024-04-25 15:34:21,336 Epoch number 23, batch number 3/10:       batch loss 0.6378781199455261
2024-04-25 15:34:21,971 Epoch number 23, batch number 4/10:       batch loss 0.6946868896484375
2024-04-25 15:34:22,634 Epoch number 23, batch number 5/10:       batch loss 0.7398105263710022
2024-04-25 15:34:23,268 Epoch number 23, batch number 6/10:       batch loss 0.5973767042160034
2024-04-25 15:34:23,900 Epoch number 23, batch number 7/10:       batch loss 0.7079352140426636
2024-04-25 15:34:24,537 Epoch number 23, batch number 8/10:       batch loss 0.7235795855522156
2024-04-25 15:34:25,160 Epoch number 23, batch number 9/10:       batch loss 0.7333713173866272
2024-04-25 15:34:26,457 Epoch number 23, batch number 0/2:       batch loss 0.6949273347854614
2024-04-25 15:34:27,213 Epoch number 23, batch number 1/2:       batch loss 0.7555853128433228
2024-04-25 15:34:27,319 Epoch: 24 	Training Loss: 0.085241
2024-04-25 15:34:27,319 Time for epoch 24 : 10 sec
2024-04-25 15:34:27,319 lr for epoch 24 is 0.01000
2024-04-25 15:34:28,895 Epoch number 24, batch number 0/10:       batch loss 0.7038643956184387
2024-04-25 15:34:29,690 Epoch number 24, batch number 1/10:       batch loss 0.785366415977478
2024-04-25 15:34:30,337 Epoch number 24, batch number 2/10:       batch loss 0.709273099899292
2024-04-25 15:34:30,971 Epoch number 24, batch number 3/10:       batch loss 0.7269226312637329
2024-04-25 15:34:31,592 Epoch number 24, batch number 4/10:       batch loss 0.6594575047492981
2024-04-25 15:34:32,216 Epoch number 24, batch number 5/10:       batch loss 0.6558817028999329
2024-04-25 15:34:32,841 Epoch number 24, batch number 6/10:       batch loss 0.7049053907394409
2024-04-25 15:34:33,517 Epoch number 24, batch number 7/10:       batch loss 0.543491005897522
2024-04-25 15:34:34,140 Epoch number 24, batch number 8/10:       batch loss 0.6413984894752502
2024-04-25 15:34:34,769 Epoch number 24, batch number 9/10:       batch loss 0.5201437473297119
2024-04-25 15:34:36,047 Epoch number 24, batch number 0/2:       batch loss 0.6211128830909729
2024-04-25 15:34:36,801 Epoch number 24, batch number 1/2:       batch loss 0.5266380906105042
2024-04-25 15:34:36,904 Epoch: 25 	Training Loss: 0.083134
2024-04-25 15:34:36,904 Time for epoch 25 : 10 sec
2024-04-25 15:34:36,904 lr for epoch 25 is 0.01000
2024-04-25 15:34:38,644 Epoch number 25, batch number 0/10:       batch loss 0.4796276092529297
2024-04-25 15:34:39,456 Epoch number 25, batch number 1/10:       batch loss 0.5352417826652527
2024-04-25 15:34:40,106 Epoch number 25, batch number 2/10:       batch loss 0.5123791694641113
2024-04-25 15:34:40,741 Epoch number 25, batch number 3/10:       batch loss 0.4666658043861389
2024-04-25 15:34:41,366 Epoch number 25, batch number 4/10:       batch loss 0.5073801279067993
2024-04-25 15:34:41,989 Epoch number 25, batch number 5/10:       batch loss 0.431530237197876
2024-04-25 15:34:42,616 Epoch number 25, batch number 6/10:       batch loss 0.46287524700164795
2024-04-25 15:34:43,238 Epoch number 25, batch number 7/10:       batch loss 0.4156706929206848
2024-04-25 15:34:43,867 Epoch number 25, batch number 8/10:       batch loss 0.41987287998199463
2024-04-25 15:34:44,497 Epoch number 25, batch number 9/10:       batch loss 0.3501057028770447
2024-04-25 15:34:45,772 Epoch number 25, batch number 0/2:       batch loss 0.4305006265640259
2024-04-25 15:34:46,517 Epoch number 25, batch number 1/2:       batch loss 0.3328165113925934
2024-04-25 15:34:46,617 Epoch: 26 	Training Loss: 0.057267
2024-04-25 15:34:46,617 Time for epoch 26 : 10 sec
2024-04-25 15:34:46,617 lr for epoch 26 is 0.01000
2024-04-25 15:34:48,167 Epoch number 26, batch number 0/10:       batch loss 0.39278465509414673
2024-04-25 15:34:48,943 Epoch number 26, batch number 1/10:       batch loss 0.38923078775405884
2024-04-25 15:34:49,603 Epoch number 26, batch number 2/10:       batch loss 0.36239510774612427
2024-04-25 15:34:50,244 Epoch number 26, batch number 3/10:       batch loss 0.38021180033683777
2024-04-25 15:34:50,902 Epoch number 26, batch number 4/10:       batch loss 0.42153823375701904
2024-04-25 15:34:51,534 Epoch number 26, batch number 5/10:       batch loss 0.3564130961894989
2024-04-25 15:34:52,169 Epoch number 26, batch number 6/10:       batch loss 0.30550581216812134
2024-04-25 15:34:52,801 Epoch number 26, batch number 7/10:       batch loss 0.3702712059020996
2024-04-25 15:34:53,433 Epoch number 26, batch number 8/10:       batch loss 0.36042076349258423
2024-04-25 15:34:54,084 Epoch number 26, batch number 9/10:       batch loss 0.3333165943622589
2024-04-25 15:34:55,301 Epoch number 26, batch number 0/2:       batch loss 0.35008928179740906
2024-04-25 15:34:56,056 Epoch number 26, batch number 1/2:       batch loss 0.36946216225624084
2024-04-25 15:34:56,169 Epoch: 27 	Training Loss: 0.045901
2024-04-25 15:34:56,169 Time for epoch 27 : 10 sec
2024-04-25 15:34:56,170 lr for epoch 27 is 0.01000
2024-04-25 15:34:57,765 Epoch number 27, batch number 0/10:       batch loss 0.35111260414123535
2024-04-25 15:34:58,518 Epoch number 27, batch number 1/10:       batch loss 0.3454345464706421
2024-04-25 15:34:59,189 Epoch number 27, batch number 2/10:       batch loss 0.36666107177734375
2024-04-25 15:34:59,816 Epoch number 27, batch number 3/10:       batch loss 0.35910046100616455
2024-04-25 15:35:00,437 Epoch number 27, batch number 4/10:       batch loss 0.36810916662216187
2024-04-25 15:35:01,069 Epoch number 27, batch number 5/10:       batch loss 0.3842915892601013
2024-04-25 15:35:01,696 Epoch number 27, batch number 6/10:       batch loss 0.3748292326927185
2024-04-25 15:35:02,371 Epoch number 27, batch number 7/10:       batch loss 0.30368250608444214
2024-04-25 15:35:03,053 Epoch number 27, batch number 8/10:       batch loss 0.3661578297615051
2024-04-25 15:35:03,678 Epoch number 27, batch number 9/10:       batch loss 0.3584260046482086
2024-04-25 15:35:04,999 Epoch number 27, batch number 0/2:       batch loss 0.3082941174507141
2024-04-25 15:35:05,751 Epoch number 27, batch number 1/2:       batch loss 0.40267664194107056
2024-04-25 15:35:05,874 Epoch: 28 	Training Loss: 0.044723
2024-04-25 15:35:05,874 Time for epoch 28 : 10 sec
2024-04-25 15:35:05,874 lr for epoch 28 is 0.01000
2024-04-25 15:35:07,547 Epoch number 28, batch number 0/10:       batch loss 0.3581618666648865
2024-04-25 15:35:08,383 Epoch number 28, batch number 1/10:       batch loss 0.36138302087783813
2024-04-25 15:35:09,079 Epoch number 28, batch number 2/10:       batch loss 0.3321317434310913
2024-04-25 15:35:09,698 Epoch number 28, batch number 3/10:       batch loss 0.3620094656944275
2024-04-25 15:35:10,338 Epoch number 28, batch number 4/10:       batch loss 0.3352121412754059
2024-04-25 15:35:10,971 Epoch number 28, batch number 5/10:       batch loss 0.3260791301727295
2024-04-25 15:35:11,612 Epoch number 28, batch number 6/10:       batch loss 0.3978382349014282
2024-04-25 15:35:12,245 Epoch number 28, batch number 7/10:       batch loss 0.3291788697242737
2024-04-25 15:35:12,868 Epoch number 28, batch number 8/10:       batch loss 0.36816906929016113
2024-04-25 15:35:13,485 Epoch number 28, batch number 9/10:       batch loss 0.3144320547580719
2024-04-25 15:35:14,828 Epoch number 28, batch number 0/2:       batch loss 0.3091464340686798
2024-04-25 15:35:15,579 Epoch number 28, batch number 1/2:       batch loss 0.38386037945747375
2024-04-25 15:35:15,699 Epoch: 29 	Training Loss: 0.043557
2024-04-25 15:35:15,699 Time for epoch 29 : 10 sec
2024-04-25 15:35:15,699 lr for epoch 29 is 0.01000
2024-04-25 15:35:17,363 Epoch number 29, batch number 0/10:       batch loss 0.3435176908969879
2024-04-25 15:35:18,175 Epoch number 29, batch number 1/10:       batch loss 0.352400541305542
2024-04-25 15:35:18,827 Epoch number 29, batch number 2/10:       batch loss 0.35290074348449707
2024-04-25 15:35:19,460 Epoch number 29, batch number 3/10:       batch loss 0.3597866892814636
2024-04-25 15:35:20,123 Epoch number 29, batch number 4/10:       batch loss 0.29701223969459534
2024-04-25 15:35:20,824 Epoch number 29, batch number 5/10:       batch loss 0.33335626125335693
2024-04-25 15:35:21,459 Epoch number 29, batch number 6/10:       batch loss 0.3542117774486542
2024-04-25 15:35:22,088 Epoch number 29, batch number 7/10:       batch loss 0.30905571579933167
2024-04-25 15:35:22,712 Epoch number 29, batch number 8/10:       batch loss 0.3685585558414459
2024-04-25 15:35:23,332 Epoch number 29, batch number 9/10:       batch loss 0.36605703830718994
2024-04-25 15:35:24,623 Epoch number 29, batch number 0/2:       batch loss 0.3424609303474426
2024-04-25 15:35:25,374 Epoch number 29, batch number 1/2:       batch loss 0.34630197286605835
2024-04-25 15:35:25,501 Epoch: 30 	Training Loss: 0.042961
2024-04-25 15:35:25,501 Time for epoch 30 : 10 sec
2024-04-25 15:35:25,501 lr for epoch 30 is 0.01000
2024-04-25 15:35:27,085 Epoch number 30, batch number 0/10:       batch loss 0.30112171173095703
2024-04-25 15:35:27,875 Epoch number 30, batch number 1/10:       batch loss 0.31882116198539734
2024-04-25 15:35:28,538 Epoch number 30, batch number 2/10:       batch loss 0.34891650080680847
2024-04-25 15:35:29,248 Epoch number 30, batch number 3/10:       batch loss 0.3709484934806824
2024-04-25 15:35:29,930 Epoch number 30, batch number 4/10:       batch loss 0.3756774365901947
2024-04-25 15:35:30,561 Epoch number 30, batch number 5/10:       batch loss 0.3467026352882385
2024-04-25 15:35:31,200 Epoch number 30, batch number 6/10:       batch loss 0.293254554271698
2024-04-25 15:35:31,828 Epoch number 30, batch number 7/10:       batch loss 0.34321796894073486
2024-04-25 15:35:32,467 Epoch number 30, batch number 8/10:       batch loss 0.34838172793388367
2024-04-25 15:35:33,093 Epoch number 30, batch number 9/10:       batch loss 0.3412103056907654
2024-04-25 15:35:34,392 Epoch number 30, batch number 0/2:       batch loss 0.34903889894485474
2024-04-25 15:35:35,142 Epoch number 30, batch number 1/2:       batch loss 0.31938648223876953
2024-04-25 15:35:35,258 Epoch: 31 	Training Loss: 0.042353
2024-04-25 15:35:35,258 Time for epoch 31 : 10 sec
2024-04-25 15:35:35,258 lr for epoch 31 is 0.01000
2024-04-25 15:35:36,862 Epoch number 31, batch number 0/10:       batch loss 0.35363492369651794
2024-04-25 15:35:37,683 Epoch number 31, batch number 1/10:       batch loss 0.3431452214717865
2024-04-25 15:35:38,335 Epoch number 31, batch number 2/10:       batch loss 0.35735470056533813
2024-04-25 15:35:38,967 Epoch number 31, batch number 3/10:       batch loss 0.30397072434425354
2024-04-25 15:35:39,587 Epoch number 31, batch number 4/10:       batch loss 0.316567063331604
2024-04-25 15:35:40,208 Epoch number 31, batch number 5/10:       batch loss 0.32026952505111694
2024-04-25 15:35:40,826 Epoch number 31, batch number 6/10:       batch loss 0.3410570025444031
2024-04-25 15:35:41,458 Epoch number 31, batch number 7/10:       batch loss 0.3382369577884674
2024-04-25 15:35:42,095 Epoch number 31, batch number 8/10:       batch loss 0.3036425709724426
2024-04-25 15:35:42,713 Epoch number 31, batch number 9/10:       batch loss 0.3090740144252777
2024-04-25 15:35:44,013 Epoch number 31, batch number 0/2:       batch loss 0.3385733664035797
2024-04-25 15:35:44,761 Epoch number 31, batch number 1/2:       batch loss 0.31746789813041687
2024-04-25 15:35:44,870 Epoch: 32 	Training Loss: 0.041087
2024-04-25 15:35:44,870 Time for epoch 32 : 10 sec
2024-04-25 15:35:44,870 lr for epoch 32 is 0.01000
2024-04-25 15:35:46,536 Epoch number 32, batch number 0/10:       batch loss 0.34356486797332764
2024-04-25 15:35:47,253 Epoch number 32, batch number 1/10:       batch loss 0.30430999398231506
2024-04-25 15:35:47,933 Epoch number 32, batch number 2/10:       batch loss 0.29790329933166504
2024-04-25 15:35:48,606 Epoch number 32, batch number 3/10:       batch loss 0.31806302070617676
2024-04-25 15:35:49,302 Epoch number 32, batch number 4/10:       batch loss 0.36891502141952515
2024-04-25 15:35:49,935 Epoch number 32, batch number 5/10:       batch loss 0.3125291168689728
2024-04-25 15:35:50,565 Epoch number 32, batch number 6/10:       batch loss 0.3309088349342346
2024-04-25 15:35:51,201 Epoch number 32, batch number 7/10:       batch loss 0.28307002782821655
2024-04-25 15:35:51,848 Epoch number 32, batch number 8/10:       batch loss 0.3809677064418793
2024-04-25 15:35:52,485 Epoch number 32, batch number 9/10:       batch loss 0.3324359655380249
2024-04-25 15:35:53,758 Epoch number 32, batch number 0/2:       batch loss 0.33230432868003845
2024-04-25 15:35:54,514 Epoch number 32, batch number 1/2:       batch loss 0.32959383726119995
2024-04-25 15:35:54,624 Epoch: 33 	Training Loss: 0.040908
2024-04-25 15:35:54,624 Time for epoch 33 : 10 sec
2024-04-25 15:35:54,624 lr for epoch 33 is 0.01000
2024-04-25 15:35:56,258 Epoch number 33, batch number 0/10:       batch loss 0.31528395414352417
2024-04-25 15:35:57,013 Epoch number 33, batch number 1/10:       batch loss 0.29409706592559814
2024-04-25 15:35:57,678 Epoch number 33, batch number 2/10:       batch loss 0.3578925132751465
2024-04-25 15:35:58,317 Epoch number 33, batch number 3/10:       batch loss 0.3317418694496155
2024-04-25 15:35:58,942 Epoch number 33, batch number 4/10:       batch loss 0.34440550208091736
2024-04-25 15:35:59,574 Epoch number 33, batch number 5/10:       batch loss 0.32382646203041077
2024-04-25 15:36:00,193 Epoch number 33, batch number 6/10:       batch loss 0.33376577496528625
2024-04-25 15:36:00,816 Epoch number 33, batch number 7/10:       batch loss 0.35744696855545044
2024-04-25 15:36:01,443 Epoch number 33, batch number 8/10:       batch loss 0.27712833881378174
2024-04-25 15:36:02,075 Epoch number 33, batch number 9/10:       batch loss 0.30432841181755066
2024-04-25 15:36:03,365 Epoch number 33, batch number 0/2:       batch loss 0.311149537563324
2024-04-25 15:36:04,113 Epoch number 33, batch number 1/2:       batch loss 0.336582750082016
2024-04-25 15:36:04,218 Epoch: 34 	Training Loss: 0.040499
2024-04-25 15:36:04,219 Time for epoch 34 : 10 sec
2024-04-25 15:36:04,219 lr for epoch 34 is 0.01000
2024-04-25 15:36:05,876 Epoch number 34, batch number 0/10:       batch loss 0.30146992206573486
2024-04-25 15:36:06,633 Epoch number 34, batch number 1/10:       batch loss 0.3163224756717682
2024-04-25 15:36:07,329 Epoch number 34, batch number 2/10:       batch loss 0.3085697889328003
2024-04-25 15:36:07,959 Epoch number 34, batch number 3/10:       batch loss 0.37206751108169556
2024-04-25 15:36:08,577 Epoch number 34, batch number 4/10:       batch loss 0.2972230613231659
2024-04-25 15:36:09,213 Epoch number 34, batch number 5/10:       batch loss 0.28312429785728455
2024-04-25 15:36:09,838 Epoch number 34, batch number 6/10:       batch loss 0.3106493651866913
2024-04-25 15:36:10,460 Epoch number 34, batch number 7/10:       batch loss 0.34585314989089966
2024-04-25 15:36:11,084 Epoch number 34, batch number 8/10:       batch loss 0.32936298847198486
2024-04-25 15:36:11,708 Epoch number 34, batch number 9/10:       batch loss 0.344809353351593
2024-04-25 15:36:13,024 Epoch number 34, batch number 0/2:       batch loss 0.34172818064689636
2024-04-25 15:36:13,775 Epoch number 34, batch number 1/2:       batch loss 0.30203860998153687
2024-04-25 15:36:13,879 Epoch: 35 	Training Loss: 0.040118
2024-04-25 15:36:13,879 Time for epoch 35 : 10 sec
2024-04-25 15:36:13,879 lr for epoch 35 is 0.01000
2024-04-25 15:36:15,473 Epoch number 35, batch number 0/10:       batch loss 0.28894999623298645
2024-04-25 15:36:16,267 Epoch number 35, batch number 1/10:       batch loss 0.2958955466747284
2024-04-25 15:36:16,932 Epoch number 35, batch number 2/10:       batch loss 0.38416045904159546
2024-04-25 15:36:17,631 Epoch number 35, batch number 3/10:       batch loss 0.32256340980529785
2024-04-25 15:36:18,259 Epoch number 35, batch number 4/10:       batch loss 0.3220604360103607
2024-04-25 15:36:18,885 Epoch number 35, batch number 5/10:       batch loss 0.30985531210899353
2024-04-25 15:36:19,509 Epoch number 35, batch number 6/10:       batch loss 0.2916721999645233
2024-04-25 15:36:20,141 Epoch number 35, batch number 7/10:       batch loss 0.3175630569458008
2024-04-25 15:36:20,778 Epoch number 35, batch number 8/10:       batch loss 0.32294848561286926
2024-04-25 15:36:21,429 Epoch number 35, batch number 9/10:       batch loss 0.33755260705947876
2024-04-25 15:36:22,820 Epoch number 35, batch number 0/2:       batch loss 0.30724477767944336
2024-04-25 15:36:23,579 Epoch number 35, batch number 1/2:       batch loss 0.3362109065055847
2024-04-25 15:36:23,696 Epoch: 36 	Training Loss: 0.039915
2024-04-25 15:36:23,697 Time for epoch 36 : 10 sec
2024-04-25 15:36:23,697 lr for epoch 36 is 0.01000
2024-04-25 15:36:25,305 Epoch number 36, batch number 0/10:       batch loss 0.34336552023887634
2024-04-25 15:36:26,295 Epoch number 36, batch number 1/10:       batch loss 0.3190189003944397
2024-04-25 15:36:26,934 Epoch number 36, batch number 2/10:       batch loss 0.35723018646240234
2024-04-25 15:36:27,593 Epoch number 36, batch number 3/10:       batch loss 0.3803684711456299
2024-04-25 15:36:28,399 Epoch number 36, batch number 4/10:       batch loss 0.37864723801612854
2024-04-25 15:36:29,253 Epoch number 36, batch number 5/10:       batch loss 0.41718730330467224
2024-04-25 15:36:30,061 Epoch number 36, batch number 6/10:       batch loss 0.3837859630584717
2024-04-25 15:36:30,855 Epoch number 36, batch number 7/10:       batch loss 0.34463775157928467
2024-04-25 15:36:31,488 Epoch number 36, batch number 8/10:       batch loss 0.36666905879974365
2024-04-25 15:36:32,107 Epoch number 36, batch number 9/10:       batch loss 0.31374338269233704
2024-04-25 15:36:33,332 Epoch number 36, batch number 0/2:       batch loss 0.34393444657325745
2024-04-25 15:36:34,082 Epoch number 36, batch number 1/2:       batch loss 0.32809770107269287
2024-04-25 15:36:34,203 Epoch: 37 	Training Loss: 0.045058
2024-04-25 15:36:34,203 Time for epoch 37 : 11 sec
2024-04-25 15:36:34,203 lr for epoch 37 is 0.01000
2024-04-25 15:36:35,746 Epoch number 37, batch number 0/10:       batch loss 0.3388078212738037
2024-04-25 15:36:36,551 Epoch number 37, batch number 1/10:       batch loss 0.3278734087944031
2024-04-25 15:36:37,196 Epoch number 37, batch number 2/10:       batch loss 0.31230229139328003
2024-04-25 15:36:37,855 Epoch number 37, batch number 3/10:       batch loss 0.29698634147644043
2024-04-25 15:36:38,522 Epoch number 37, batch number 4/10:       batch loss 0.3298299014568329
2024-04-25 15:36:39,151 Epoch number 37, batch number 5/10:       batch loss 0.3367329239845276
2024-04-25 15:36:39,784 Epoch number 37, batch number 6/10:       batch loss 0.30496224761009216
2024-04-25 15:36:40,406 Epoch number 37, batch number 7/10:       batch loss 0.3016470670700073
2024-04-25 15:36:41,030 Epoch number 37, batch number 8/10:       batch loss 0.33514055609703064
2024-04-25 15:36:41,651 Epoch number 37, batch number 9/10:       batch loss 0.3492765724658966
2024-04-25 15:36:42,940 Epoch number 37, batch number 0/2:       batch loss 0.34409913420677185
2024-04-25 15:36:43,706 Epoch number 37, batch number 1/2:       batch loss 0.31326302886009216
2024-04-25 15:36:43,825 Epoch: 38 	Training Loss: 0.040419
2024-04-25 15:36:43,825 Time for epoch 38 : 10 sec
2024-04-25 15:36:43,825 lr for epoch 38 is 0.01000
2024-04-25 15:36:45,413 Epoch number 38, batch number 0/10:       batch loss 0.32750970125198364
2024-04-25 15:36:46,170 Epoch number 38, batch number 1/10:       batch loss 0.29329097270965576
2024-04-25 15:36:46,816 Epoch number 38, batch number 2/10:       batch loss 0.3204154074192047
2024-04-25 15:36:47,486 Epoch number 38, batch number 3/10:       batch loss 0.32577016949653625
2024-04-25 15:36:48,292 Epoch number 38, batch number 4/10:       batch loss 0.41041597723960876
2024-04-25 15:36:49,099 Epoch number 38, batch number 5/10:       batch loss 0.39602601528167725
2024-04-25 15:36:49,888 Epoch number 38, batch number 6/10:       batch loss 0.3947056829929352
2024-04-25 15:36:50,670 Epoch number 38, batch number 7/10:       batch loss 0.36967384815216064
2024-04-25 15:36:51,457 Epoch number 38, batch number 8/10:       batch loss 0.37828609347343445
2024-04-25 15:36:52,241 Epoch number 38, batch number 9/10:       batch loss 0.37662243843078613
2024-04-25 15:36:53,623 Epoch number 38, batch number 0/2:       batch loss 0.37198612093925476
2024-04-25 15:36:54,423 Epoch number 38, batch number 1/2:       batch loss 0.3689454197883606
2024-04-25 15:36:54,506 Epoch: 39 	Training Loss: 0.044909
2024-04-25 15:36:54,506 Time for epoch 39 : 11 sec
2024-04-25 15:36:54,506 lr for epoch 39 is 0.01000
2024-04-25 15:36:56,291 Epoch number 39, batch number 0/10:       batch loss 0.31673842668533325
2024-04-25 15:36:57,247 Epoch number 39, batch number 1/10:       batch loss 0.39631688594818115
2024-04-25 15:36:58,090 Epoch number 39, batch number 2/10:       batch loss 0.3690420687198639
2024-04-25 15:36:58,892 Epoch number 39, batch number 3/10:       batch loss 0.3647797405719757
2024-04-25 15:36:59,698 Epoch number 39, batch number 4/10:       batch loss 0.38426443934440613
2024-04-25 15:37:00,495 Epoch number 39, batch number 5/10:       batch loss 0.38151001930236816
2024-04-25 15:37:01,295 Epoch number 39, batch number 6/10:       batch loss 0.3418986201286316
2024-04-25 15:37:02,111 Epoch number 39, batch number 7/10:       batch loss 0.4007919430732727
2024-04-25 15:37:02,941 Epoch number 39, batch number 8/10:       batch loss 0.4302521049976349
2024-04-25 15:37:03,742 Epoch number 39, batch number 9/10:       batch loss 0.4284937083721161
2024-04-25 15:37:05,061 Epoch number 39, batch number 0/2:       batch loss 0.44462841749191284
2024-04-25 15:37:05,863 Epoch number 39, batch number 1/2:       batch loss 0.37660133838653564
2024-04-25 15:37:05,973 Epoch: 40 	Training Loss: 0.047676
2024-04-25 15:37:05,973 Time for epoch 40 : 11 sec
2024-04-25 15:37:05,973 lr for epoch 40 is 0.01000
2024-04-25 15:37:15,283 Epoch number 0, batch number 0/2:       batch loss 0.40623003244400024
2024-04-25 15:37:16,904 Epoch number 0, batch number 1/2:       batch loss 0.40208467841148376
2024-04-25 15:37:16,918 Epoch: 1 	Training Loss: 0.050520
2024-04-25 15:37:16,918 Time for epoch 1 : 8 sec
2024-04-25 15:37:16,918 lr for epoch 1 is 0.01000
2024-04-25 15:37:17,843 Epoch number 0, batch number 0/2:       batch loss 0.41940438747406006
2024-04-25 15:37:18,636 Epoch number 0, batch number 1/2:       batch loss 0.3724786341190338
2024-04-25 15:37:24,936 Epoch number 1, batch number 0/2:       batch loss 0.4026736617088318
2024-04-25 15:37:26,550 Epoch number 1, batch number 1/2:       batch loss 0.36368152499198914
2024-04-25 15:37:26,577 Epoch: 2 	Training Loss: 0.047897
2024-04-25 15:37:26,577 Time for epoch 2 : 8 sec
2024-04-25 15:37:26,577 lr for epoch 2 is 0.01000
2024-04-25 15:37:27,540 Epoch number 1, batch number 0/2:       batch loss 0.3644779622554779
2024-04-25 15:37:28,331 Epoch number 1, batch number 1/2:       batch loss 0.4159732460975647
2024-04-25 15:37:34,704 Epoch number 2, batch number 0/2:       batch loss 0.3878912329673767
2024-04-25 15:37:36,344 Epoch number 2, batch number 1/2:       batch loss 0.3895615339279175
2024-04-25 15:37:36,365 Epoch: 3 	Training Loss: 0.048591
2024-04-25 15:37:36,365 Time for epoch 3 : 8 sec
2024-04-25 15:37:36,365 lr for epoch 3 is 0.01000
2024-04-25 15:37:37,275 Epoch number 2, batch number 0/2:       batch loss 0.35486871004104614
2024-04-25 15:37:38,065 Epoch number 2, batch number 1/2:       batch loss 0.4207812547683716
2024-04-25 15:37:44,457 Epoch number 3, batch number 0/2:       batch loss 0.3847527801990509
2024-04-25 15:37:46,094 Epoch number 3, batch number 1/2:       batch loss 0.38422712683677673
2024-04-25 15:37:46,119 Epoch: 4 	Training Loss: 0.048061
2024-04-25 15:37:46,119 Time for epoch 4 : 8 sec
2024-04-25 15:37:46,119 lr for epoch 4 is 0.01000
2024-04-25 15:37:47,018 Epoch number 3, batch number 0/2:       batch loss 0.3982156813144684
2024-04-25 15:37:47,809 Epoch number 3, batch number 1/2:       batch loss 0.3661024868488312
2024-04-25 15:37:54,133 Epoch number 4, batch number 0/2:       batch loss 0.38350847363471985
2024-04-25 15:37:55,779 Epoch number 4, batch number 1/2:       batch loss 0.347974956035614
2024-04-25 15:37:55,800 Epoch: 5 	Training Loss: 0.045718
2024-04-25 15:37:55,800 Time for epoch 5 : 8 sec
2024-04-25 15:37:55,801 lr for epoch 5 is 0.01000
2024-04-25 15:37:56,689 Epoch number 4, batch number 0/2:       batch loss 0.3635737895965576
2024-04-25 15:37:57,480 Epoch number 4, batch number 1/2:       batch loss 0.37437868118286133
2024-04-25 15:38:03,827 Epoch number 5, batch number 0/2:       batch loss 0.37087902426719666
2024-04-25 15:38:05,452 Epoch number 5, batch number 1/2:       batch loss 0.3546716570854187
2024-04-25 15:38:05,478 Epoch: 6 	Training Loss: 0.045347
2024-04-25 15:38:05,478 Time for epoch 6 : 8 sec
2024-04-25 15:38:05,478 lr for epoch 6 is 0.01000
2024-04-25 15:38:06,347 Epoch number 5, batch number 0/2:       batch loss 0.41180843114852905
2024-04-25 15:38:07,141 Epoch number 5, batch number 1/2:       batch loss 0.36963316798210144
2024-04-25 15:38:13,443 Epoch number 6, batch number 0/2:       batch loss 0.3868942856788635
2024-04-25 15:38:15,075 Epoch number 6, batch number 1/2:       batch loss 0.35986632108688354
2024-04-25 15:38:15,101 Epoch: 7 	Training Loss: 0.046673
2024-04-25 15:38:15,101 Time for epoch 7 : 8 sec
2024-04-25 15:38:15,101 lr for epoch 7 is 0.01000
2024-04-25 15:38:15,961 Epoch number 6, batch number 0/2:       batch loss 0.3903677761554718
2024-04-25 15:38:16,750 Epoch number 6, batch number 1/2:       batch loss 0.36538368463516235
2024-04-25 15:38:23,122 Epoch number 7, batch number 0/2:       batch loss 0.3714849650859833
2024-04-25 15:38:24,769 Epoch number 7, batch number 1/2:       batch loss 0.37366193532943726
2024-04-25 15:38:24,795 Epoch: 8 	Training Loss: 0.046572
2024-04-25 15:38:24,796 Time for epoch 8 : 8 sec
2024-04-25 15:38:24,796 lr for epoch 8 is 0.01000
2024-04-25 15:38:25,672 Epoch number 7, batch number 0/2:       batch loss 0.37608426809310913
2024-04-25 15:38:26,429 Epoch number 7, batch number 1/2:       batch loss 0.34945106506347656
2024-04-25 15:38:33,024 Epoch number 8, batch number 0/2:       batch loss 0.36105191707611084
2024-04-25 15:38:34,662 Epoch number 8, batch number 1/2:       batch loss 0.3469372093677521
2024-04-25 15:38:34,688 Epoch: 9 	Training Loss: 0.044249
2024-04-25 15:38:34,688 Time for epoch 9 : 8 sec
2024-04-25 15:38:34,688 lr for epoch 9 is 0.01000
2024-04-25 15:38:35,610 Epoch number 8, batch number 0/2:       batch loss 0.3180093765258789
2024-04-25 15:38:36,405 Epoch number 8, batch number 1/2:       batch loss 0.40887513756752014
2024-04-25 15:38:42,802 Epoch number 9, batch number 0/2:       batch loss 0.3598741888999939
2024-04-25 15:38:44,445 Epoch number 9, batch number 1/2:       batch loss 0.3561300039291382
2024-04-25 15:38:44,463 Epoch: 10 	Training Loss: 0.044750
2024-04-25 15:38:44,464 Time for epoch 10 : 8 sec
2024-04-25 15:38:44,464 lr for epoch 10 is 0.01000
2024-04-25 15:38:45,344 Epoch number 9, batch number 0/2:       batch loss 0.3666975796222687
2024-04-25 15:38:46,136 Epoch number 9, batch number 1/2:       batch loss 0.35543960332870483
2024-04-25 15:39:13,237 findfont: Font family 'Arial' not found.
2024-04-25 15:39:13,237 findfont: Font family 'Arial' not found.
2024-04-25 15:39:13,237 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:39:13,237 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:39:13,244 findfont: Font family 'Arial' not found.
2024-04-25 15:39:13,244 findfont: Font family 'Arial' not found.
2024-04-25 15:39:13,248 findfont: Font family 'Arial' not found.
2024-04-25 15:39:13,249 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:39:41,760 findfont: Font family 'Arial' not found.
2024-04-25 15:39:41,760 findfont: Font family 'Arial' not found.
2024-04-25 15:39:41,760 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:39:41,760 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:39:41,767 findfont: Font family 'Arial' not found.
2024-04-25 15:39:41,767 findfont: Font family 'Arial' not found.
2024-04-25 15:39:41,771 findfont: Font family 'Arial' not found.
2024-04-25 15:39:41,773 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:40:10,121 findfont: Font family 'Arial' not found.
2024-04-25 15:40:10,121 findfont: Font family 'Arial' not found.
2024-04-25 15:40:10,122 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:40:10,122 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:40:10,128 findfont: Font family 'Arial' not found.
2024-04-25 15:40:10,128 findfont: Font family 'Arial' not found.
2024-04-25 15:40:10,132 findfont: Font family 'Arial' not found.
2024-04-25 15:40:10,134 findfont: Font family 'Times New Roman' not found.
2024-04-25 15:40:17,494 Run Finished Successfully
